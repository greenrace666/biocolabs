{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greenrace666/biocolabs/blob/main/proteindiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tu2xP8ZfdNY",
        "outputId": "e4da6ac6-1591-426d-fc73-df9f4ef08228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 220ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install diffusers BioPython torch_geometric hf_xet --prerelease disallow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall torch-cluster\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch._y_version__}.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "vHLptT18JySF",
        "outputId": "10f73299-f3a3-4bbe-d517-617bac841941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch_cluster 1.6.3+pt26cu124\n",
            "Uninstalling torch_cluster-1.6.3+pt26cu124:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/torch_cluster-1.6.3+pt26cu124.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch_cluster/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torch_cluster-1.6.3+pt26cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-cluster\n",
            "  Using cached https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (2.0.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt26cu124\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch_cluster"
                ]
              },
              "id": "51d97c9becfe4fb489fa82d84ced07b8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsppByerX8zX",
        "outputId": "ccdef6fd-fe1f-4285-ca5b-3b7361d7162a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 10 mmCIF files...\n",
            "Successfully downloaded 4PQB.cif\n",
            "Successfully downloaded 8R39.cif\n",
            "Successfully downloaded 4JIT.cif\n",
            "Successfully downloaded 8F5U.cif\n",
            "Successfully downloaded 2Q8L.cif\n",
            "Successfully downloaded 1AIP.cif\n",
            "Successfully downloaded 1CJA.cif\n",
            "Successfully downloaded 8XSG.cif\n",
            "Successfully downloaded 1G9I.cif\n",
            "Successfully downloaded 7ESG.cif\n",
            "\n",
            "Download complete! Successfully downloaded 10 out of 10 files.\n",
            "Files are saved in the 'data' directory.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import random\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def get_random_pdb_ids(num_ids=10):\n",
        "    \"\"\"Get a list of random PDB IDs from RCSB.\"\"\"\n",
        "    # Use RCSB REST API to get a list of all PDB IDs\n",
        "    url = \"https://data.rcsb.org/rest/v1/holdings/current/entry_ids\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        all_pdb_ids = response.json()\n",
        "        # Randomly select num_ids from the list\n",
        "        return random.sample(all_pdb_ids, num_ids)\n",
        "    else:\n",
        "        raise Exception(f\"Failed to get PDB IDs. Status code: {response.status_code}\")\n",
        "\n",
        "def download_mmcif(pdb_id, output_dir):\n",
        "    \"\"\"Download mmCIF file for a given PDB ID.\"\"\"\n",
        "    # Create URL for mmCIF file\n",
        "    url = f\"https://files.rcsb.org/download/{pdb_id}.cif\"\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Set output file path\n",
        "    output_file = output_dir / f\"{pdb_id}.cif\"\n",
        "\n",
        "    try:\n",
        "        # Download the file\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Write the file\n",
        "        with open(output_file, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"Successfully downloaded {pdb_id}.cif\")\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading {pdb_id}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    # Set the output directory\n",
        "    output_dir = \"data\"\n",
        "\n",
        "    try:\n",
        "        # Get 10 random PDB IDs\n",
        "        pdb_ids = get_random_pdb_ids(10)\n",
        "\n",
        "        print(f\"Downloading {len(pdb_ids)} mmCIF files...\")\n",
        "\n",
        "        # Download each mmCIF file\n",
        "        successful_downloads = 0\n",
        "        for pdb_id in pdb_ids:\n",
        "            if download_mmcif(pdb_id, output_dir):\n",
        "                successful_downloads += 1\n",
        "\n",
        "        print(f\"\\nDownload complete! Successfully downloaded {successful_downloads} out of {len(pdb_ids)} files.\")\n",
        "        print(f\"Files are saved in the '{output_dir}' directory.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------GPU----------------------------------\n",
        "# Import statements\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from transformers import EsmModel, EsmTokenizer\n",
        "from diffusers import DDPMScheduler\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import knn_graph\n",
        "import numpy as np\n",
        "import pickle\n",
        "from Bio.PDB import MMCIFParser, PPBuilder\n",
        "import argparse\n",
        "import torch_cluster\n",
        "\n",
        "# Placeholder for SE(3)-equivariant GNN layer\n",
        "class EGNNLayer(nn.Module):\n",
        "    def __init__(self, in_scalar_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.scalar_net = nn.Linear(in_scalar_dim, hidden_dim)\n",
        "        self.vector_net = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, h, x, edge_index):\n",
        "        h = F.relu(self.scalar_net(h))\n",
        "        # Simplified: Actual EGNN updates coordinates (x) with equivariant message passing\n",
        "        return h, x\n",
        "\n",
        "class GraphUNet(nn.Module):\n",
        "    def __init__(self, in_scalar_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.down1 = EGNNLayer(in_scalar_dim, hidden_dim)\n",
        "        self.down2 = EGNNLayer(hidden_dim, hidden_dim)\n",
        "        self.bottleneck = EGNNLayer(hidden_dim, hidden_dim)\n",
        "        self.up2 = EGNNLayer(hidden_dim * 2, hidden_dim)  # Concatenated skip connection\n",
        "        self.up1 = EGNNLayer(hidden_dim * 2, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, 3)  # Predict noise for each atom (x, y, z)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x_scalar, x_vector, edge_index = data.x_scalar, data.x_vector, data.edge_index\n",
        "\n",
        "        h1, _ = checkpoint(self.down1, x_scalar, x_vector, edge_index)\n",
        "        h2, _ = checkpoint(self.down2, h1, x_vector, edge_index)\n",
        "        h_b, _ = checkpoint(self.bottleneck, h2, x_vector, edge_index)\n",
        "        h_up2, _ = checkpoint(self.up2, torch.cat([h_b, h2], dim=1), x_vector, edge_index)\n",
        "        h_up1, _ = checkpoint(self.up1, torch.cat([h_up2, h1], dim=1), x_vector, edge_index)\n",
        "\n",
        "        pred_epsilon = self.out(h_up1)\n",
        "        return pred_epsilon\n",
        "\n",
        "# Dataset class for loading preprocessed protein data\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data = []\n",
        "        for file in os.listdir(data_dir):\n",
        "            if file.endswith('.pkl'):\n",
        "                with open(os.path.join(data_dir, file), 'rb') as f:\n",
        "                    self.data.append(pickle.load(f))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Function to parse CIF files and extract all-atom coordinates and sequence\n",
        "def parse_cif(cif_file):\n",
        "    parser = MMCIFParser(QUIET=True)\n",
        "    structure = parser.get_structure('protein', cif_file)\n",
        "    ppb = PPBuilder()\n",
        "    peptides = ppb.build_peptides(structure)\n",
        "    sequence = ''.join([str(pp.get_sequence()) for pp in ppb.build_peptides(structure)])\n",
        "    seq_residues = []\n",
        "    for pp in peptides:\n",
        "        seq_residues.extend(pp)\n",
        "\n",
        "    # Create a set of residue IDs for filtering\n",
        "    seq_residue_ids = set(res.get_id() for res in seq_residues)\n",
        "    coords = []\n",
        "    valid_atoms=[]\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "              if residue.get_id() in seq_residue_ids:\n",
        "                for atom in residue:\n",
        "                    coords.append(atom.get_coord())\n",
        "                    valid_atoms.append(residue.get_id())\n",
        "\n",
        "    coords = torch.tensor(np.array(coords), dtype=torch.float32)\n",
        "    return sequence, coords, valid_atoms\n",
        "\n",
        "# Preprocess data: compute ESM-2 embeddings and save with coordinates\n",
        "def preprocess_data(data_dir):\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t30_150M_UR50D\")\n",
        "    esm_model = EsmModel.from_pretrained(\"facebook/esm2_t30_150M_UR50D\").eval()\n",
        "    print(f\"ESM-2 hidden size: {esm_model.config.hidden_size}\")  # Should be 1280\n",
        "\n",
        "    cif_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.cif')]\n",
        "    if not cif_files:\n",
        "        print(f\"No .cif files found in {data_dir}\")\n",
        "        return\n",
        "\n",
        "    for cif_file in cif_files:\n",
        "        output_file = os.path.join(data_dir, os.path.basename(cif_file).replace('.cif', '.pkl'))\n",
        "        if os.path.exists(output_file):\n",
        "            print(f\"Skipping {cif_file}, already preprocessed.\")\n",
        "            continue\n",
        "        print(f\"Processing {cif_file}\")\n",
        "        sequence, coords, valid_atoms = parse_cif(cif_file)\n",
        "        inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = esm_model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state[0]  # [seq_len, 1280]\n",
        "        print(f\"Embeddings shape before mapping: {embeddings.shape}\")  # Debug\n",
        "\n",
        "        seq_residues = []\n",
        "        for pp in PPBuilder().build_peptides(MMCIFParser().get_structure('protein', cif_file)):\n",
        "            seq_residues.extend(pp)\n",
        "        residue_id_to_idx = {res.get_id(): idx for idx, res in enumerate(seq_residues)}\n",
        "\n",
        "        atom_to_residue = []\n",
        "        for res_id in valid_atoms:\n",
        "            if res_id in residue_id_to_idx:\n",
        "                atom_to_residue.append(residue_id_to_idx[res_id])\n",
        "            else:\n",
        "                print(f\"Warning: Residue ID {res_id} not found in sequence for {cif_file}\")\n",
        "\n",
        "        if not atom_to_residue:\n",
        "            print(f\"Error: No valid atom-to-residue mappings for {cif_file}\")\n",
        "            continue\n",
        "        if max(atom_to_residue, default=-1) >= len(embeddings):\n",
        "            print(f\"Error: Residue index out of bounds for {cif_file}\")\n",
        "            continue\n",
        "        embeddings = embeddings[atom_to_residue]\n",
        "        print(f\"Embeddings shape after mapping: {embeddings.shape}\")  # Should be [num_atoms, 1280]\n",
        "        if embeddings.shape[-1] != 640:\n",
        "            print(f\"Error: Expected 640 dimensions, got {embeddings.shape[-1]} for {cif_file}\")\n",
        "            continue\n",
        "        data = Data(\n",
        "            x_scalar=embeddings,\n",
        "            x_vector=coords,\n",
        "            edge_index=knn_graph(coords, k=6)\n",
        "        )\n",
        "        with open(output_file, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"Saved preprocessed data to {output_file}\")\n",
        "# Timestep embedding for diffusion model\n",
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(-torch.linspace(0, 1, half, device=timesteps.device) * np.log(max_period))\n",
        "    args = timesteps[:, None].float() * freqs[None]\n",
        "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    return embedding\n",
        "\n",
        "# Training function\n",
        "def train(args):\n",
        "    device = torch.device('cuda' if args.device == 'gpu' and torch.cuda.is_available() else 'cpu')\n",
        "    dataset = ProteinDataset(args.data_dir)\n",
        "    if len(dataset) == 0:\n",
        "        print(f\"No preprocessed .pkl files found in {args.data_dir}. Please run preprocessing first.\")\n",
        "        return\n",
        "    loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=Batch.from_data_list)\n",
        "    model = GraphUNet(in_scalar_dim=640 + 32, hidden_dim=256).to(device)\n",
        "    scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "    scheduler.alphas_cumprod = scheduler.alphas_cumprod.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scaler = GradScaler(enabled=(args.device == 'gpu'))\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            print(f\"batch.x_scalar shape: {batch.x_scalar.shape}\")  # Debug\n",
        "            t = torch.randint(0, scheduler.config.num_train_timesteps, (batch.num_graphs,), device=device)\n",
        "            alpha_bar = scheduler.alphas_cumprod[t]\n",
        "            alpha_bar_per_atom = alpha_bar[batch.batch]\n",
        "            sqrt_alpha_bar = torch.sqrt(alpha_bar_per_atom)\n",
        "            sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar_per_atom)\n",
        "            noise = torch.randn_like(batch.x_vector)\n",
        "            x_t = sqrt_alpha_bar[:, None] * batch.x_vector + sqrt_one_minus_alpha_bar[:, None] * noise\n",
        "            t_emb = timestep_embedding(t, dim=32).to(device)\n",
        "            print(f\"t_emb_per_atom shape: {t_emb[batch.batch].shape}\")  # Debug\n",
        "            t_emb_per_atom = t_emb[batch.batch]\n",
        "            x_scalar = torch.cat([batch.x_scalar, t_emb_per_atom], dim=1)\n",
        "            print(f\"x_scalar shape: {x_scalar.shape}\")  # Debug\n",
        "            data = Data(x_scalar=x_scalar, x_vector=x_t, edge_index=batch.edge_index)\n",
        "            with autocast(device_type='cuda', enabled=(args.device == 'gpu')):\n",
        "                pred_epsilon = model(data)\n",
        "                loss = F.mse_loss(pred_epsilon, noise)\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        print(f\"Epoch {epoch + 1}/{args.epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "        # Save standard checkpoint\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        # Update checkpoint_path\n",
        "        checkpoint_path = os.path.join('/content/drive/MyDrive/', f\"model_epoch_{epoch + 1}.pth\")\n",
        "        jit_checkpoint_path = os.path.join('/content/drive/MyDrive/', f\"model_epoch_{epoch + 1}_jit.pt\")\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Saved standard checkpoint to {checkpoint_path}\")\n",
        "\n",
        "        # Save TorchScript checkpoint\n",
        "        model.eval()  # JIT requires eval mode\n",
        "        try:\n",
        "            # Create example input for tracing\n",
        "            example_data = Data(\n",
        "                x_scalar=torch.randn(100, 640 + 32, device=device),\n",
        "                x_vector=torch.randn(100, 3, device=device),\n",
        "                edge_index=torch.randint(0, 100, (2, 200), device=device)\n",
        "            )\n",
        "            # Trace the model\n",
        "            traced_model = torch.jit.trace(model, example_data)\n",
        "            traced_model.save(jit_checkpoint_path)\n",
        "            print(f\"Saved TorchScript checkpoint to {jit_checkpoint_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save TorchScript checkpoint: {e}\")\n",
        "        model.train()\n",
        "        # Set up arguments\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.data_dir = 'data'\n",
        "        self.device = 'gpu'\n",
        "        self.distributed = False\n",
        "        self.epochs = 100  # For testing\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Preprocess CIF files in the data directory\n",
        "preprocess_data(args.data_dir)\n",
        "\n",
        "# Run training\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyoPQiOIysGp",
        "outputId": "2edb75fa-f110-49e2-81db-59ac6ff85cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ESM-2 hidden size: 640\n",
            "Processing data/4PQB.cif\n",
            "Embeddings shape before mapping: torch.Size([155, 640])\n",
            "Embeddings shape after mapping: torch.Size([1216, 640])\n",
            "Saved preprocessed data to data/4PQB.pkl\n",
            "Processing data/8F5U.cif\n",
            "Embeddings shape before mapping: torch.Size([2074, 640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 15947.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 15984.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 16011.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain D is discontinuous at line 16036.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 16059.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 16327.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 16572.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain D is discontinuous at line 16838.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape after mapping: torch.Size([15901, 640])\n",
            "Saved preprocessed data to data/8F5U.pkl\n",
            "Processing data/1AIP.cif\n",
            "Embeddings shape before mapping: torch.Size([2271, 640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 17560.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 17562.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain D is discontinuous at line 17563.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain E is discontinuous at line 17567.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain G is discontinuous at line 17570.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain H is discontinuous at line 17574.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape after mapping: torch.Size([17560, 640])\n",
            "Saved preprocessed data to data/1AIP.pkl\n",
            "Processing data/4JIT.cif\n",
            "Embeddings shape before mapping: torch.Size([593, 640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 4511.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 4534.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 4557.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 4558.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape after mapping: torch.Size([4511, 640])\n",
            "Saved preprocessed data to data/4JIT.pkl\n",
            "Processing data/2Q8L.cif\n",
            "Embeddings shape before mapping: torch.Size([316, 640])\n",
            "Embeddings shape after mapping: torch.Size([2593, 640])\n",
            "Saved preprocessed data to data/2Q8L.pkl\n",
            "Processing data/8R39.cif\n",
            "Embeddings shape before mapping: torch.Size([210, 640])\n",
            "Embeddings shape after mapping: torch.Size([3172, 640])\n",
            "Saved preprocessed data to data/8R39.pkl\n",
            "Processing data/1G9I.cif\n",
            "Embeddings shape before mapping: torch.Size([247, 640])\n",
            "Embeddings shape after mapping: torch.Size([1791, 640])\n",
            "Saved preprocessed data to data/1G9I.pkl\n",
            "Processing data/1CJA.cif\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain E is discontinuous at line 1791.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain I is discontinuous at line 1974.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape before mapping: torch.Size([656, 640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 6240.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 6263.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape after mapping: torch.Size([6240, 640])\n",
            "Saved preprocessed data to data/1CJA.pkl\n",
            "Processing data/7ESG.cif\n",
            "Embeddings shape before mapping: torch.Size([591, 640])\n",
            "Embeddings shape after mapping: torch.Size([4514, 640])\n",
            "Saved preprocessed data to data/7ESG.pkl\n",
            "Processing data/8XSG.cif\n",
            "Embeddings shape before mapping: torch.Size([344, 640])\n",
            "Embeddings shape after mapping: torch.Size([5553, 640])\n",
            "Saved preprocessed data to data/8XSG.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index', 'x_scalar', 'x_vector'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch.x_scalar shape: torch.Size([30907, 640])\n",
            "t_emb_per_atom shape: torch.Size([30907, 32])\n",
            "x_scalar shape: torch.Size([30907, 672])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch.x_scalar shape: torch.Size([13071, 640])\n",
            "t_emb_per_atom shape: torch.Size([13071, 32])\n",
            "x_scalar shape: torch.Size([13071, 672])\n",
            "batch.x_scalar shape: torch.Size([19073, 640])\n",
            "t_emb_per_atom shape: torch.Size([19073, 32])\n",
            "x_scalar shape: torch.Size([19073, 672])\n",
            "Epoch 1/100, Loss: 1.012090802192688\n",
            "Mounted at /content/drive\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_1.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([16516, 640])\n",
            "t_emb_per_atom shape: torch.Size([16516, 32])\n",
            "x_scalar shape: torch.Size([16516, 672])\n",
            "batch.x_scalar shape: torch.Size([27184, 640])\n",
            "t_emb_per_atom shape: torch.Size([27184, 32])\n",
            "x_scalar shape: torch.Size([27184, 672])\n",
            "batch.x_scalar shape: torch.Size([19351, 640])\n",
            "t_emb_per_atom shape: torch.Size([19351, 32])\n",
            "x_scalar shape: torch.Size([19351, 672])\n",
            "Epoch 2/100, Loss: 1.0107563734054565\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_2.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([10693, 640])\n",
            "t_emb_per_atom shape: torch.Size([10693, 32])\n",
            "x_scalar shape: torch.Size([10693, 672])\n",
            "batch.x_scalar shape: torch.Size([41607, 640])\n",
            "t_emb_per_atom shape: torch.Size([41607, 32])\n",
            "x_scalar shape: torch.Size([41607, 672])\n",
            "batch.x_scalar shape: torch.Size([10751, 640])\n",
            "t_emb_per_atom shape: torch.Size([10751, 32])\n",
            "x_scalar shape: torch.Size([10751, 672])\n",
            "Epoch 3/100, Loss: 1.0099586248397827\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_3.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([25842, 640])\n",
            "t_emb_per_atom shape: torch.Size([25842, 32])\n",
            "x_scalar shape: torch.Size([25842, 672])\n",
            "batch.x_scalar shape: torch.Size([13409, 640])\n",
            "t_emb_per_atom shape: torch.Size([13409, 32])\n",
            "x_scalar shape: torch.Size([13409, 672])\n",
            "batch.x_scalar shape: torch.Size([23800, 640])\n",
            "t_emb_per_atom shape: torch.Size([23800, 32])\n",
            "x_scalar shape: torch.Size([23800, 672])\n",
            "Epoch 4/100, Loss: 1.0007981061935425\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_4.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([42873, 640])\n",
            "t_emb_per_atom shape: torch.Size([42873, 32])\n",
            "x_scalar shape: torch.Size([42873, 672])\n",
            "batch.x_scalar shape: torch.Size([12032, 640])\n",
            "t_emb_per_atom shape: torch.Size([12032, 32])\n",
            "x_scalar shape: torch.Size([12032, 672])\n",
            "batch.x_scalar shape: torch.Size([8146, 640])\n",
            "t_emb_per_atom shape: torch.Size([8146, 32])\n",
            "x_scalar shape: torch.Size([8146, 672])\n",
            "Epoch 5/100, Loss: 1.0081654787063599\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_5.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([16177, 640])\n",
            "t_emb_per_atom shape: torch.Size([16177, 32])\n",
            "x_scalar shape: torch.Size([16177, 672])\n",
            "batch.x_scalar shape: torch.Size([13413, 640])\n",
            "t_emb_per_atom shape: torch.Size([13413, 32])\n",
            "x_scalar shape: torch.Size([13413, 672])\n",
            "batch.x_scalar shape: torch.Size([33461, 640])\n",
            "t_emb_per_atom shape: torch.Size([33461, 32])\n",
            "x_scalar shape: torch.Size([33461, 672])\n",
            "Epoch 6/100, Loss: 0.9970930218696594\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_6.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([17171, 640])\n",
            "t_emb_per_atom shape: torch.Size([17171, 32])\n",
            "x_scalar shape: torch.Size([17171, 672])\n",
            "batch.x_scalar shape: torch.Size([12419, 640])\n",
            "t_emb_per_atom shape: torch.Size([12419, 32])\n",
            "x_scalar shape: torch.Size([12419, 672])\n",
            "batch.x_scalar shape: torch.Size([33461, 640])\n",
            "t_emb_per_atom shape: torch.Size([33461, 32])\n",
            "x_scalar shape: torch.Size([33461, 672])\n",
            "Epoch 7/100, Loss: 1.0059452056884766\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_7.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([11495, 640])\n",
            "t_emb_per_atom shape: torch.Size([11495, 32])\n",
            "x_scalar shape: torch.Size([11495, 672])\n",
            "batch.x_scalar shape: torch.Size([29485, 640])\n",
            "t_emb_per_atom shape: torch.Size([29485, 32])\n",
            "x_scalar shape: torch.Size([29485, 672])\n",
            "batch.x_scalar shape: torch.Size([22071, 640])\n",
            "t_emb_per_atom shape: torch.Size([22071, 32])\n",
            "x_scalar shape: torch.Size([22071, 672])\n",
            "Epoch 8/100, Loss: 1.0094119310379028\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_8.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([40568, 640])\n",
            "t_emb_per_atom shape: torch.Size([40568, 32])\n",
            "x_scalar shape: torch.Size([40568, 672])\n",
            "batch.x_scalar shape: torch.Size([13758, 640])\n",
            "t_emb_per_atom shape: torch.Size([13758, 32])\n",
            "x_scalar shape: torch.Size([13758, 672])\n",
            "batch.x_scalar shape: torch.Size([8725, 640])\n",
            "t_emb_per_atom shape: torch.Size([8725, 32])\n",
            "x_scalar shape: torch.Size([8725, 672])\n",
            "Epoch 9/100, Loss: 1.0053203105926514\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_9.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([39191, 640])\n",
            "t_emb_per_atom shape: torch.Size([39191, 32])\n",
            "x_scalar shape: torch.Size([39191, 672])\n",
            "batch.x_scalar shape: torch.Size([13109, 640])\n",
            "t_emb_per_atom shape: torch.Size([13109, 32])\n",
            "x_scalar shape: torch.Size([13109, 672])\n",
            "batch.x_scalar shape: torch.Size([10751, 640])\n",
            "t_emb_per_atom shape: torch.Size([10751, 32])\n",
            "x_scalar shape: torch.Size([10751, 672])\n",
            "Epoch 10/100, Loss: 1.017079472541809\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_10.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([37845, 640])\n",
            "t_emb_per_atom shape: torch.Size([37845, 32])\n",
            "x_scalar shape: torch.Size([37845, 672])\n",
            "batch.x_scalar shape: torch.Size([15139, 640])\n",
            "t_emb_per_atom shape: torch.Size([15139, 32])\n",
            "x_scalar shape: torch.Size([15139, 672])\n",
            "batch.x_scalar shape: torch.Size([10067, 640])\n",
            "t_emb_per_atom shape: torch.Size([10067, 32])\n",
            "x_scalar shape: torch.Size([10067, 672])\n",
            "Epoch 11/100, Loss: 0.9990285038948059\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_11.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([18437, 640])\n",
            "t_emb_per_atom shape: torch.Size([18437, 32])\n",
            "x_scalar shape: torch.Size([18437, 672])\n",
            "batch.x_scalar shape: torch.Size([23160, 640])\n",
            "t_emb_per_atom shape: torch.Size([23160, 32])\n",
            "x_scalar shape: torch.Size([23160, 672])\n",
            "batch.x_scalar shape: torch.Size([21454, 640])\n",
            "t_emb_per_atom shape: torch.Size([21454, 32])\n",
            "x_scalar shape: torch.Size([21454, 672])\n",
            "Epoch 12/100, Loss: 1.0021330118179321\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_12.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([13413, 640])\n",
            "t_emb_per_atom shape: torch.Size([13413, 32])\n",
            "x_scalar shape: torch.Size([13413, 672])\n",
            "batch.x_scalar shape: torch.Size([31144, 640])\n",
            "t_emb_per_atom shape: torch.Size([31144, 32])\n",
            "x_scalar shape: torch.Size([31144, 672])\n",
            "batch.x_scalar shape: torch.Size([18494, 640])\n",
            "t_emb_per_atom shape: torch.Size([18494, 32])\n",
            "x_scalar shape: torch.Size([18494, 672])\n",
            "Epoch 13/100, Loss: 1.0037304162979126\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_13.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([27756, 640])\n",
            "t_emb_per_atom shape: torch.Size([27756, 32])\n",
            "x_scalar shape: torch.Size([27756, 672])\n",
            "batch.x_scalar shape: torch.Size([31486, 640])\n",
            "t_emb_per_atom shape: torch.Size([31486, 32])\n",
            "x_scalar shape: torch.Size([31486, 672])\n",
            "batch.x_scalar shape: torch.Size([3809, 640])\n",
            "t_emb_per_atom shape: torch.Size([3809, 32])\n",
            "x_scalar shape: torch.Size([3809, 672])\n",
            "Epoch 14/100, Loss: 1.001538634300232\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_14.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([11840, 640])\n",
            "t_emb_per_atom shape: torch.Size([11840, 32])\n",
            "x_scalar shape: torch.Size([11840, 672])\n",
            "batch.x_scalar shape: torch.Size([43528, 640])\n",
            "t_emb_per_atom shape: torch.Size([43528, 32])\n",
            "x_scalar shape: torch.Size([43528, 672])\n",
            "batch.x_scalar shape: torch.Size([7683, 640])\n",
            "t_emb_per_atom shape: torch.Size([7683, 32])\n",
            "x_scalar shape: torch.Size([7683, 672])\n",
            "Epoch 15/100, Loss: 1.0133414268493652\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_15.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([27104, 640])\n",
            "t_emb_per_atom shape: torch.Size([27104, 32])\n",
            "x_scalar shape: torch.Size([27104, 672])\n",
            "batch.x_scalar shape: torch.Size([30217, 640])\n",
            "t_emb_per_atom shape: torch.Size([30217, 32])\n",
            "x_scalar shape: torch.Size([30217, 672])\n",
            "batch.x_scalar shape: torch.Size([5730, 640])\n",
            "t_emb_per_atom shape: torch.Size([5730, 32])\n",
            "x_scalar shape: torch.Size([5730, 672])\n",
            "Epoch 16/100, Loss: 1.0087038278579712\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_16.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([40917, 640])\n",
            "t_emb_per_atom shape: torch.Size([40917, 32])\n",
            "x_scalar shape: torch.Size([40917, 672])\n",
            "batch.x_scalar shape: torch.Size([14448, 640])\n",
            "t_emb_per_atom shape: torch.Size([14448, 32])\n",
            "x_scalar shape: torch.Size([14448, 672])\n",
            "batch.x_scalar shape: torch.Size([7686, 640])\n",
            "t_emb_per_atom shape: torch.Size([7686, 32])\n",
            "x_scalar shape: torch.Size([7686, 672])\n",
            "Epoch 17/100, Loss: 1.005162000656128\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_17.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([26455, 640])\n",
            "t_emb_per_atom shape: torch.Size([26455, 32])\n",
            "x_scalar shape: torch.Size([26455, 672])\n",
            "batch.x_scalar shape: torch.Size([24803, 640])\n",
            "t_emb_per_atom shape: torch.Size([24803, 32])\n",
            "x_scalar shape: torch.Size([24803, 672])\n",
            "batch.x_scalar shape: torch.Size([11793, 640])\n",
            "t_emb_per_atom shape: torch.Size([11793, 32])\n",
            "x_scalar shape: torch.Size([11793, 672])\n",
            "Epoch 18/100, Loss: 0.9983572959899902\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_18.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28763, 640])\n",
            "t_emb_per_atom shape: torch.Size([28763, 32])\n",
            "x_scalar shape: torch.Size([28763, 672])\n",
            "batch.x_scalar shape: torch.Size([17171, 640])\n",
            "t_emb_per_atom shape: torch.Size([17171, 32])\n",
            "x_scalar shape: torch.Size([17171, 672])\n",
            "batch.x_scalar shape: torch.Size([17117, 640])\n",
            "t_emb_per_atom shape: torch.Size([17117, 32])\n",
            "x_scalar shape: torch.Size([17117, 672])\n",
            "Epoch 19/100, Loss: 0.9994479417800903\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_19.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([30904, 640])\n",
            "t_emb_per_atom shape: torch.Size([30904, 32])\n",
            "x_scalar shape: torch.Size([30904, 672])\n",
            "batch.x_scalar shape: torch.Size([23422, 640])\n",
            "t_emb_per_atom shape: torch.Size([23422, 32])\n",
            "x_scalar shape: torch.Size([23422, 672])\n",
            "batch.x_scalar shape: torch.Size([8725, 640])\n",
            "t_emb_per_atom shape: torch.Size([8725, 32])\n",
            "x_scalar shape: torch.Size([8725, 672])\n",
            "Epoch 20/100, Loss: 0.9994708299636841\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_20.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([18437, 640])\n",
            "t_emb_per_atom shape: torch.Size([18437, 32])\n",
            "x_scalar shape: torch.Size([18437, 672])\n",
            "batch.x_scalar shape: torch.Size([25838, 640])\n",
            "t_emb_per_atom shape: torch.Size([25838, 32])\n",
            "x_scalar shape: torch.Size([25838, 672])\n",
            "batch.x_scalar shape: torch.Size([18776, 640])\n",
            "t_emb_per_atom shape: torch.Size([18776, 32])\n",
            "x_scalar shape: torch.Size([18776, 672])\n",
            "Epoch 21/100, Loss: 1.0049034357070923\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_21.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([29245, 640])\n",
            "t_emb_per_atom shape: torch.Size([29245, 32])\n",
            "x_scalar shape: torch.Size([29245, 672])\n",
            "batch.x_scalar shape: torch.Size([23739, 640])\n",
            "t_emb_per_atom shape: torch.Size([23739, 32])\n",
            "x_scalar shape: torch.Size([23739, 672])\n",
            "batch.x_scalar shape: torch.Size([10067, 640])\n",
            "t_emb_per_atom shape: torch.Size([10067, 32])\n",
            "x_scalar shape: torch.Size([10067, 672])\n",
            "Epoch 22/100, Loss: 1.008184552192688\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_22.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([31144, 640])\n",
            "t_emb_per_atom shape: torch.Size([31144, 32])\n",
            "x_scalar shape: torch.Size([31144, 672])\n",
            "batch.x_scalar shape: torch.Size([24803, 640])\n",
            "t_emb_per_atom shape: torch.Size([24803, 32])\n",
            "x_scalar shape: torch.Size([24803, 672])\n",
            "batch.x_scalar shape: torch.Size([7104, 640])\n",
            "t_emb_per_atom shape: torch.Size([7104, 32])\n",
            "x_scalar shape: torch.Size([7104, 672])\n",
            "Epoch 23/100, Loss: 1.0086545944213867\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_23.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([15138, 640])\n",
            "t_emb_per_atom shape: torch.Size([15138, 32])\n",
            "x_scalar shape: torch.Size([15138, 672])\n",
            "batch.x_scalar shape: torch.Size([24800, 640])\n",
            "t_emb_per_atom shape: torch.Size([24800, 32])\n",
            "x_scalar shape: torch.Size([24800, 672])\n",
            "batch.x_scalar shape: torch.Size([23113, 640])\n",
            "t_emb_per_atom shape: torch.Size([23113, 32])\n",
            "x_scalar shape: torch.Size([23113, 672])\n",
            "Epoch 24/100, Loss: 1.009341835975647\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_24.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([39766, 640])\n",
            "t_emb_per_atom shape: torch.Size([39766, 32])\n",
            "x_scalar shape: torch.Size([39766, 672])\n",
            "batch.x_scalar shape: torch.Size([18897, 640])\n",
            "t_emb_per_atom shape: torch.Size([18897, 32])\n",
            "x_scalar shape: torch.Size([18897, 672])\n",
            "batch.x_scalar shape: torch.Size([4388, 640])\n",
            "t_emb_per_atom shape: torch.Size([4388, 32])\n",
            "x_scalar shape: torch.Size([4388, 672])\n",
            "Epoch 25/100, Loss: 1.0179427862167358\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_25.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([25950, 640])\n",
            "t_emb_per_atom shape: torch.Size([25950, 32])\n",
            "x_scalar shape: torch.Size([25950, 672])\n",
            "batch.x_scalar shape: torch.Size([16369, 640])\n",
            "t_emb_per_atom shape: torch.Size([16369, 32])\n",
            "x_scalar shape: torch.Size([16369, 672])\n",
            "batch.x_scalar shape: torch.Size([20732, 640])\n",
            "t_emb_per_atom shape: torch.Size([20732, 32])\n",
            "x_scalar shape: torch.Size([20732, 672])\n",
            "Epoch 26/100, Loss: 1.0036745071411133\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_26.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([31483, 640])\n",
            "t_emb_per_atom shape: torch.Size([31483, 32])\n",
            "x_scalar shape: torch.Size([31483, 672])\n",
            "batch.x_scalar shape: torch.Size([23422, 640])\n",
            "t_emb_per_atom shape: torch.Size([23422, 32])\n",
            "x_scalar shape: torch.Size([23422, 672])\n",
            "batch.x_scalar shape: torch.Size([8146, 640])\n",
            "t_emb_per_atom shape: torch.Size([8146, 32])\n",
            "x_scalar shape: torch.Size([8146, 672])\n",
            "Epoch 27/100, Loss: 1.0009461641311646\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_27.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([10111, 640])\n",
            "t_emb_per_atom shape: torch.Size([10111, 32])\n",
            "x_scalar shape: torch.Size([10111, 672])\n",
            "batch.x_scalar shape: torch.Size([45254, 640])\n",
            "t_emb_per_atom shape: torch.Size([45254, 32])\n",
            "x_scalar shape: torch.Size([45254, 672])\n",
            "batch.x_scalar shape: torch.Size([7686, 640])\n",
            "t_emb_per_atom shape: torch.Size([7686, 32])\n",
            "x_scalar shape: torch.Size([7686, 672])\n",
            "Epoch 28/100, Loss: 0.9816470742225647\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_28.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([31483, 640])\n",
            "t_emb_per_atom shape: torch.Size([31483, 32])\n",
            "x_scalar shape: torch.Size([31483, 672])\n",
            "batch.x_scalar shape: torch.Size([24224, 640])\n",
            "t_emb_per_atom shape: torch.Size([24224, 32])\n",
            "x_scalar shape: torch.Size([24224, 672])\n",
            "batch.x_scalar shape: torch.Size([7344, 640])\n",
            "t_emb_per_atom shape: torch.Size([7344, 32])\n",
            "x_scalar shape: torch.Size([7344, 672])\n",
            "Epoch 29/100, Loss: 0.9938122630119324\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_29.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([11840, 640])\n",
            "t_emb_per_atom shape: torch.Size([11840, 32])\n",
            "x_scalar shape: torch.Size([11840, 672])\n",
            "batch.x_scalar shape: torch.Size([29757, 640])\n",
            "t_emb_per_atom shape: torch.Size([29757, 32])\n",
            "x_scalar shape: torch.Size([29757, 672])\n",
            "batch.x_scalar shape: torch.Size([21454, 640])\n",
            "t_emb_per_atom shape: torch.Size([21454, 32])\n",
            "x_scalar shape: torch.Size([21454, 672])\n",
            "Epoch 30/100, Loss: 1.0042048692703247\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_30.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([18900, 640])\n",
            "t_emb_per_atom shape: torch.Size([18900, 32])\n",
            "x_scalar shape: torch.Size([18900, 672])\n",
            "batch.x_scalar shape: torch.Size([23739, 640])\n",
            "t_emb_per_atom shape: torch.Size([23739, 32])\n",
            "x_scalar shape: torch.Size([23739, 672])\n",
            "batch.x_scalar shape: torch.Size([20412, 640])\n",
            "t_emb_per_atom shape: torch.Size([20412, 32])\n",
            "x_scalar shape: torch.Size([20412, 672])\n",
            "Epoch 31/100, Loss: 1.0041792392730713\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_31.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([41607, 640])\n",
            "t_emb_per_atom shape: torch.Size([41607, 32])\n",
            "x_scalar shape: torch.Size([41607, 672])\n",
            "batch.x_scalar shape: torch.Size([16481, 640])\n",
            "t_emb_per_atom shape: torch.Size([16481, 32])\n",
            "x_scalar shape: torch.Size([16481, 672])\n",
            "batch.x_scalar shape: torch.Size([4963, 640])\n",
            "t_emb_per_atom shape: torch.Size([4963, 32])\n",
            "x_scalar shape: torch.Size([4963, 672])\n",
            "Epoch 32/100, Loss: 0.9775072932243347\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_32.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([18437, 640])\n",
            "t_emb_per_atom shape: torch.Size([18437, 32])\n",
            "x_scalar shape: torch.Size([18437, 672])\n",
            "batch.x_scalar shape: torch.Size([40805, 640])\n",
            "t_emb_per_atom shape: torch.Size([40805, 32])\n",
            "x_scalar shape: torch.Size([40805, 672])\n",
            "batch.x_scalar shape: torch.Size([3809, 640])\n",
            "t_emb_per_atom shape: torch.Size([3809, 32])\n",
            "x_scalar shape: torch.Size([3809, 672])\n",
            "Epoch 33/100, Loss: 1.0164217948913574\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_33.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([10690, 640])\n",
            "t_emb_per_atom shape: torch.Size([10690, 32])\n",
            "x_scalar shape: torch.Size([10690, 672])\n",
            "batch.x_scalar shape: torch.Size([28561, 640])\n",
            "t_emb_per_atom shape: torch.Size([28561, 32])\n",
            "x_scalar shape: torch.Size([28561, 672])\n",
            "batch.x_scalar shape: torch.Size([23800, 640])\n",
            "t_emb_per_atom shape: torch.Size([23800, 32])\n",
            "x_scalar shape: torch.Size([23800, 672])\n",
            "Epoch 34/100, Loss: 0.9992860555648804\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_34.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([25838, 640])\n",
            "t_emb_per_atom shape: torch.Size([25838, 32])\n",
            "x_scalar shape: torch.Size([25838, 672])\n",
            "batch.x_scalar shape: torch.Size([16481, 640])\n",
            "t_emb_per_atom shape: torch.Size([16481, 32])\n",
            "x_scalar shape: torch.Size([16481, 672])\n",
            "batch.x_scalar shape: torch.Size([20732, 640])\n",
            "t_emb_per_atom shape: torch.Size([20732, 32])\n",
            "x_scalar shape: torch.Size([20732, 672])\n",
            "Epoch 35/100, Loss: 0.9997326731681824\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_35.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([41607, 640])\n",
            "t_emb_per_atom shape: torch.Size([41607, 32])\n",
            "x_scalar shape: torch.Size([41607, 672])\n",
            "batch.x_scalar shape: torch.Size([10690, 640])\n",
            "t_emb_per_atom shape: torch.Size([10690, 32])\n",
            "x_scalar shape: torch.Size([10690, 672])\n",
            "batch.x_scalar shape: torch.Size([10754, 640])\n",
            "t_emb_per_atom shape: torch.Size([10754, 32])\n",
            "x_scalar shape: torch.Size([10754, 672])\n",
            "Epoch 36/100, Loss: 1.004169225692749\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_36.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([10690, 640])\n",
            "t_emb_per_atom shape: torch.Size([10690, 32])\n",
            "x_scalar shape: torch.Size([10690, 672])\n",
            "batch.x_scalar shape: torch.Size([45254, 640])\n",
            "t_emb_per_atom shape: torch.Size([45254, 32])\n",
            "x_scalar shape: torch.Size([45254, 672])\n",
            "batch.x_scalar shape: torch.Size([7107, 640])\n",
            "t_emb_per_atom shape: torch.Size([7107, 32])\n",
            "x_scalar shape: torch.Size([7107, 672])\n",
            "Epoch 37/100, Loss: 0.9835911989212036\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_37.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([22080, 640])\n",
            "t_emb_per_atom shape: torch.Size([22080, 32])\n",
            "x_scalar shape: torch.Size([22080, 672])\n",
            "batch.x_scalar shape: torch.Size([32138, 640])\n",
            "t_emb_per_atom shape: torch.Size([32138, 32])\n",
            "x_scalar shape: torch.Size([32138, 672])\n",
            "batch.x_scalar shape: torch.Size([8833, 640])\n",
            "t_emb_per_atom shape: torch.Size([8833, 32])\n",
            "x_scalar shape: torch.Size([8833, 672])\n",
            "Epoch 38/100, Loss: 1.0069698095321655\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_38.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([39226, 640])\n",
            "t_emb_per_atom shape: torch.Size([39226, 32])\n",
            "x_scalar shape: torch.Size([39226, 672])\n",
            "batch.x_scalar shape: torch.Size([16481, 640])\n",
            "t_emb_per_atom shape: torch.Size([16481, 32])\n",
            "x_scalar shape: torch.Size([16481, 672])\n",
            "batch.x_scalar shape: torch.Size([7344, 640])\n",
            "t_emb_per_atom shape: torch.Size([7344, 32])\n",
            "x_scalar shape: torch.Size([7344, 672])\n",
            "Epoch 39/100, Loss: 1.0112441778182983\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_39.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([25078, 640])\n",
            "t_emb_per_atom shape: torch.Size([25078, 32])\n",
            "x_scalar shape: torch.Size([25078, 672])\n",
            "batch.x_scalar shape: torch.Size([27219, 640])\n",
            "t_emb_per_atom shape: torch.Size([27219, 32])\n",
            "x_scalar shape: torch.Size([27219, 672])\n",
            "batch.x_scalar shape: torch.Size([10754, 640])\n",
            "t_emb_per_atom shape: torch.Size([10754, 32])\n",
            "x_scalar shape: torch.Size([10754, 672])\n",
            "Epoch 40/100, Loss: 0.9940261840820312\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_40.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([30217, 640])\n",
            "t_emb_per_atom shape: torch.Size([30217, 32])\n",
            "x_scalar shape: torch.Size([30217, 672])\n",
            "batch.x_scalar shape: torch.Size([24803, 640])\n",
            "t_emb_per_atom shape: torch.Size([24803, 32])\n",
            "x_scalar shape: torch.Size([24803, 672])\n",
            "batch.x_scalar shape: torch.Size([8031, 640])\n",
            "t_emb_per_atom shape: torch.Size([8031, 32])\n",
            "x_scalar shape: torch.Size([8031, 672])\n",
            "Epoch 41/100, Loss: 0.9849307537078857\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_41.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([16516, 640])\n",
            "t_emb_per_atom shape: torch.Size([16516, 32])\n",
            "x_scalar shape: torch.Size([16516, 672])\n",
            "batch.x_scalar shape: torch.Size([40805, 640])\n",
            "t_emb_per_atom shape: torch.Size([40805, 32])\n",
            "x_scalar shape: torch.Size([40805, 672])\n",
            "batch.x_scalar shape: torch.Size([5730, 640])\n",
            "t_emb_per_atom shape: torch.Size([5730, 32])\n",
            "x_scalar shape: torch.Size([5730, 672])\n",
            "Epoch 42/100, Loss: 1.0114870071411133\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_42.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28558, 640])\n",
            "t_emb_per_atom shape: torch.Size([28558, 32])\n",
            "x_scalar shape: torch.Size([28558, 672])\n",
            "batch.x_scalar shape: torch.Size([26462, 640])\n",
            "t_emb_per_atom shape: torch.Size([26462, 32])\n",
            "x_scalar shape: torch.Size([26462, 672])\n",
            "batch.x_scalar shape: torch.Size([8031, 640])\n",
            "t_emb_per_atom shape: torch.Size([8031, 32])\n",
            "x_scalar shape: torch.Size([8031, 672])\n",
            "Epoch 43/100, Loss: 1.009874939918518\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_43.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([26177, 640])\n",
            "t_emb_per_atom shape: torch.Size([26177, 32])\n",
            "x_scalar shape: torch.Size([26177, 672])\n",
            "batch.x_scalar shape: torch.Size([25081, 640])\n",
            "t_emb_per_atom shape: torch.Size([25081, 32])\n",
            "x_scalar shape: torch.Size([25081, 672])\n",
            "batch.x_scalar shape: torch.Size([11793, 640])\n",
            "t_emb_per_atom shape: torch.Size([11793, 32])\n",
            "x_scalar shape: torch.Size([11793, 672])\n",
            "Epoch 44/100, Loss: 0.9857147336006165\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_44.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([24799, 640])\n",
            "t_emb_per_atom shape: torch.Size([24799, 32])\n",
            "x_scalar shape: torch.Size([24799, 672])\n",
            "batch.x_scalar shape: torch.Size([32525, 640])\n",
            "t_emb_per_atom shape: torch.Size([32525, 32])\n",
            "x_scalar shape: torch.Size([32525, 672])\n",
            "batch.x_scalar shape: torch.Size([5727, 640])\n",
            "t_emb_per_atom shape: torch.Size([5727, 32])\n",
            "x_scalar shape: torch.Size([5727, 672])\n",
            "Epoch 45/100, Loss: 0.9930459856987\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_45.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([24796, 640])\n",
            "t_emb_per_atom shape: torch.Size([24796, 32])\n",
            "x_scalar shape: torch.Size([24796, 672])\n",
            "batch.x_scalar shape: torch.Size([16181, 640])\n",
            "t_emb_per_atom shape: torch.Size([16181, 32])\n",
            "x_scalar shape: torch.Size([16181, 672])\n",
            "batch.x_scalar shape: torch.Size([22074, 640])\n",
            "t_emb_per_atom shape: torch.Size([22074, 32])\n",
            "x_scalar shape: torch.Size([22074, 672])\n",
            "Epoch 46/100, Loss: 1.0083335638046265\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_46.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28561, 640])\n",
            "t_emb_per_atom shape: torch.Size([28561, 32])\n",
            "x_scalar shape: torch.Size([28561, 672])\n",
            "batch.x_scalar shape: torch.Size([30102, 640])\n",
            "t_emb_per_atom shape: torch.Size([30102, 32])\n",
            "x_scalar shape: torch.Size([30102, 672])\n",
            "batch.x_scalar shape: torch.Size([4388, 640])\n",
            "t_emb_per_atom shape: torch.Size([4388, 32])\n",
            "x_scalar shape: torch.Size([4388, 672])\n",
            "Epoch 47/100, Loss: 1.0086414813995361\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_47.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([36468, 640])\n",
            "t_emb_per_atom shape: torch.Size([36468, 32])\n",
            "x_scalar shape: torch.Size([36468, 672])\n",
            "batch.x_scalar shape: torch.Size([18897, 640])\n",
            "t_emb_per_atom shape: torch.Size([18897, 32])\n",
            "x_scalar shape: torch.Size([18897, 672])\n",
            "batch.x_scalar shape: torch.Size([7686, 640])\n",
            "t_emb_per_atom shape: torch.Size([7686, 32])\n",
            "x_scalar shape: torch.Size([7686, 672])\n",
            "Epoch 48/100, Loss: 1.0085995197296143\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_48.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([23422, 640])\n",
            "t_emb_per_atom shape: torch.Size([23422, 32])\n",
            "x_scalar shape: torch.Size([23422, 672])\n",
            "batch.x_scalar shape: torch.Size([18897, 640])\n",
            "t_emb_per_atom shape: torch.Size([18897, 32])\n",
            "x_scalar shape: torch.Size([18897, 672])\n",
            "batch.x_scalar shape: torch.Size([20732, 640])\n",
            "t_emb_per_atom shape: torch.Size([20732, 32])\n",
            "x_scalar shape: torch.Size([20732, 672])\n",
            "Epoch 49/100, Loss: 0.9994065761566162\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_49.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([22882, 640])\n",
            "t_emb_per_atom shape: torch.Size([22882, 32])\n",
            "x_scalar shape: torch.Size([22882, 672])\n",
            "batch.x_scalar shape: torch.Size([20818, 640])\n",
            "t_emb_per_atom shape: torch.Size([20818, 32])\n",
            "x_scalar shape: torch.Size([20818, 672])\n",
            "batch.x_scalar shape: torch.Size([19351, 640])\n",
            "t_emb_per_atom shape: torch.Size([19351, 32])\n",
            "x_scalar shape: torch.Size([19351, 672])\n",
            "Epoch 50/100, Loss: 1.0045729875564575\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_50.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([12834, 640])\n",
            "t_emb_per_atom shape: torch.Size([12834, 32])\n",
            "x_scalar shape: torch.Size([12834, 672])\n",
            "batch.x_scalar shape: torch.Size([16756, 640])\n",
            "t_emb_per_atom shape: torch.Size([16756, 32])\n",
            "x_scalar shape: torch.Size([16756, 672])\n",
            "batch.x_scalar shape: torch.Size([33461, 640])\n",
            "t_emb_per_atom shape: torch.Size([33461, 32])\n",
            "x_scalar shape: torch.Size([33461, 672])\n",
            "Epoch 51/100, Loss: 0.9969822764396667\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_51.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28446, 640])\n",
            "t_emb_per_atom shape: torch.Size([28446, 32])\n",
            "x_scalar shape: torch.Size([28446, 672])\n",
            "batch.x_scalar shape: torch.Size([12534, 640])\n",
            "t_emb_per_atom shape: torch.Size([12534, 32])\n",
            "x_scalar shape: torch.Size([12534, 672])\n",
            "batch.x_scalar shape: torch.Size([22071, 640])\n",
            "t_emb_per_atom shape: torch.Size([22071, 32])\n",
            "x_scalar shape: torch.Size([22071, 672])\n",
            "Epoch 52/100, Loss: 0.997230052947998\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_52.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([12032, 640])\n",
            "t_emb_per_atom shape: torch.Size([12032, 32])\n",
            "x_scalar shape: torch.Size([12032, 672])\n",
            "batch.x_scalar shape: torch.Size([27906, 640])\n",
            "t_emb_per_atom shape: torch.Size([27906, 32])\n",
            "x_scalar shape: torch.Size([27906, 672])\n",
            "batch.x_scalar shape: torch.Size([23113, 640])\n",
            "t_emb_per_atom shape: torch.Size([23113, 32])\n",
            "x_scalar shape: torch.Size([23113, 672])\n",
            "Epoch 53/100, Loss: 0.997384250164032\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_53.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([32208, 640])\n",
            "t_emb_per_atom shape: torch.Size([32208, 32])\n",
            "x_scalar shape: torch.Size([32208, 672])\n",
            "batch.x_scalar shape: torch.Size([25880, 640])\n",
            "t_emb_per_atom shape: torch.Size([25880, 32])\n",
            "x_scalar shape: torch.Size([25880, 672])\n",
            "batch.x_scalar shape: torch.Size([4963, 640])\n",
            "t_emb_per_atom shape: torch.Size([4963, 32])\n",
            "x_scalar shape: torch.Size([4963, 672])\n",
            "Epoch 54/100, Loss: 1.0233805179595947\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_54.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([24221, 640])\n",
            "t_emb_per_atom shape: torch.Size([24221, 32])\n",
            "x_scalar shape: torch.Size([24221, 672])\n",
            "batch.x_scalar shape: torch.Size([18098, 640])\n",
            "t_emb_per_atom shape: torch.Size([18098, 32])\n",
            "x_scalar shape: torch.Size([18098, 672])\n",
            "batch.x_scalar shape: torch.Size([20732, 640])\n",
            "t_emb_per_atom shape: torch.Size([20732, 32])\n",
            "x_scalar shape: torch.Size([20732, 672])\n",
            "Epoch 55/100, Loss: 1.00285804271698\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_55.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([12067, 640])\n",
            "t_emb_per_atom shape: torch.Size([12067, 32])\n",
            "x_scalar shape: torch.Size([12067, 672])\n",
            "batch.x_scalar shape: torch.Size([28843, 640])\n",
            "t_emb_per_atom shape: torch.Size([28843, 32])\n",
            "x_scalar shape: torch.Size([28843, 672])\n",
            "batch.x_scalar shape: torch.Size([22141, 640])\n",
            "t_emb_per_atom shape: torch.Size([22141, 32])\n",
            "x_scalar shape: torch.Size([22141, 672])\n",
            "Epoch 56/100, Loss: 1.0071862936019897\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_56.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([17558, 640])\n",
            "t_emb_per_atom shape: torch.Size([17558, 32])\n",
            "x_scalar shape: torch.Size([17558, 672])\n",
            "batch.x_scalar shape: torch.Size([25078, 640])\n",
            "t_emb_per_atom shape: torch.Size([25078, 32])\n",
            "x_scalar shape: torch.Size([25078, 672])\n",
            "batch.x_scalar shape: torch.Size([20415, 640])\n",
            "t_emb_per_atom shape: torch.Size([20415, 32])\n",
            "x_scalar shape: torch.Size([20415, 672])\n",
            "Epoch 57/100, Loss: 1.0109832286834717\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_57.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28840, 640])\n",
            "t_emb_per_atom shape: torch.Size([28840, 32])\n",
            "x_scalar shape: torch.Size([28840, 672])\n",
            "batch.x_scalar shape: torch.Size([13796, 640])\n",
            "t_emb_per_atom shape: torch.Size([13796, 32])\n",
            "x_scalar shape: torch.Size([13796, 672])\n",
            "batch.x_scalar shape: torch.Size([20415, 640])\n",
            "t_emb_per_atom shape: torch.Size([20415, 32])\n",
            "x_scalar shape: torch.Size([20415, 672])\n",
            "Epoch 58/100, Loss: 1.0087028741836548\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_58.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([18437, 640])\n",
            "t_emb_per_atom shape: torch.Size([18437, 32])\n",
            "x_scalar shape: torch.Size([18437, 672])\n",
            "batch.x_scalar shape: torch.Size([37845, 640])\n",
            "t_emb_per_atom shape: torch.Size([37845, 32])\n",
            "x_scalar shape: torch.Size([37845, 672])\n",
            "batch.x_scalar shape: torch.Size([6769, 640])\n",
            "t_emb_per_atom shape: torch.Size([6769, 32])\n",
            "x_scalar shape: torch.Size([6769, 672])\n",
            "Epoch 59/100, Loss: 0.9907615184783936\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_59.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([39226, 640])\n",
            "t_emb_per_atom shape: torch.Size([39226, 32])\n",
            "x_scalar shape: torch.Size([39226, 672])\n",
            "batch.x_scalar shape: torch.Size([13761, 640])\n",
            "t_emb_per_atom shape: torch.Size([13761, 32])\n",
            "x_scalar shape: torch.Size([13761, 672])\n",
            "batch.x_scalar shape: torch.Size([10064, 640])\n",
            "t_emb_per_atom shape: torch.Size([10064, 32])\n",
            "x_scalar shape: torch.Size([10064, 672])\n",
            "Epoch 60/100, Loss: 1.0038522481918335\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_60.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28840, 640])\n",
            "t_emb_per_atom shape: torch.Size([28840, 32])\n",
            "x_scalar shape: torch.Size([28840, 672])\n",
            "batch.x_scalar shape: torch.Size([26180, 640])\n",
            "t_emb_per_atom shape: torch.Size([26180, 32])\n",
            "x_scalar shape: torch.Size([26180, 672])\n",
            "batch.x_scalar shape: torch.Size([8031, 640])\n",
            "t_emb_per_atom shape: torch.Size([8031, 32])\n",
            "x_scalar shape: torch.Size([8031, 672])\n",
            "Epoch 61/100, Loss: 0.9958693385124207\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_61.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([37270, 640])\n",
            "t_emb_per_atom shape: torch.Size([37270, 32])\n",
            "x_scalar shape: torch.Size([37270, 672])\n",
            "batch.x_scalar shape: torch.Size([18098, 640])\n",
            "t_emb_per_atom shape: torch.Size([18098, 32])\n",
            "x_scalar shape: torch.Size([18098, 672])\n",
            "batch.x_scalar shape: torch.Size([7683, 640])\n",
            "t_emb_per_atom shape: torch.Size([7683, 32])\n",
            "x_scalar shape: torch.Size([7683, 672])\n",
            "Epoch 62/100, Loss: 1.0115339756011963\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_62.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([26455, 640])\n",
            "t_emb_per_atom shape: torch.Size([26455, 32])\n",
            "x_scalar shape: torch.Size([26455, 672])\n",
            "batch.x_scalar shape: torch.Size([16181, 640])\n",
            "t_emb_per_atom shape: torch.Size([16181, 32])\n",
            "x_scalar shape: torch.Size([16181, 672])\n",
            "batch.x_scalar shape: torch.Size([20415, 640])\n",
            "t_emb_per_atom shape: torch.Size([20415, 32])\n",
            "x_scalar shape: torch.Size([20415, 672])\n",
            "Epoch 63/100, Loss: 1.0025970935821533\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_63.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([25078, 640])\n",
            "t_emb_per_atom shape: torch.Size([25078, 32])\n",
            "x_scalar shape: torch.Size([25078, 672])\n",
            "batch.x_scalar shape: torch.Size([32208, 640])\n",
            "t_emb_per_atom shape: torch.Size([32208, 32])\n",
            "x_scalar shape: torch.Size([32208, 672])\n",
            "batch.x_scalar shape: torch.Size([5765, 640])\n",
            "t_emb_per_atom shape: torch.Size([5765, 32])\n",
            "x_scalar shape: torch.Size([5765, 672])\n",
            "Epoch 64/100, Loss: 1.0103294849395752\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_64.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([40230, 640])\n",
            "t_emb_per_atom shape: torch.Size([40230, 32])\n",
            "x_scalar shape: torch.Size([40230, 672])\n",
            "batch.x_scalar shape: torch.Size([16519, 640])\n",
            "t_emb_per_atom shape: torch.Size([16519, 32])\n",
            "x_scalar shape: torch.Size([16519, 672])\n",
            "batch.x_scalar shape: torch.Size([6302, 640])\n",
            "t_emb_per_atom shape: torch.Size([6302, 32])\n",
            "x_scalar shape: torch.Size([6302, 672])\n",
            "Epoch 65/100, Loss: 1.0035762786865234\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_65.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([26455, 640])\n",
            "t_emb_per_atom shape: torch.Size([26455, 32])\n",
            "x_scalar shape: torch.Size([26455, 672])\n",
            "batch.x_scalar shape: torch.Size([32208, 640])\n",
            "t_emb_per_atom shape: torch.Size([32208, 32])\n",
            "x_scalar shape: torch.Size([32208, 672])\n",
            "batch.x_scalar shape: torch.Size([4388, 640])\n",
            "t_emb_per_atom shape: torch.Size([4388, 32])\n",
            "x_scalar shape: torch.Size([4388, 672])\n",
            "Epoch 66/100, Loss: 0.9972856044769287\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_66.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([15829, 640])\n",
            "t_emb_per_atom shape: torch.Size([15829, 32])\n",
            "x_scalar shape: torch.Size([15829, 672])\n",
            "batch.x_scalar shape: torch.Size([25081, 640])\n",
            "t_emb_per_atom shape: torch.Size([25081, 32])\n",
            "x_scalar shape: torch.Size([25081, 672])\n",
            "batch.x_scalar shape: torch.Size([22141, 640])\n",
            "t_emb_per_atom shape: torch.Size([22141, 32])\n",
            "x_scalar shape: torch.Size([22141, 672])\n",
            "Epoch 67/100, Loss: 0.9959031343460083\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_67.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([17171, 640])\n",
            "t_emb_per_atom shape: torch.Size([17171, 32])\n",
            "x_scalar shape: torch.Size([17171, 672])\n",
            "batch.x_scalar shape: torch.Size([12419, 640])\n",
            "t_emb_per_atom shape: torch.Size([12419, 32])\n",
            "x_scalar shape: torch.Size([12419, 672])\n",
            "batch.x_scalar shape: torch.Size([33461, 640])\n",
            "t_emb_per_atom shape: torch.Size([33461, 32])\n",
            "x_scalar shape: torch.Size([33461, 672])\n",
            "Epoch 68/100, Loss: 0.9963365793228149\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_68.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([29565, 640])\n",
            "t_emb_per_atom shape: torch.Size([29565, 32])\n",
            "x_scalar shape: torch.Size([29565, 672])\n",
            "batch.x_scalar shape: torch.Size([24461, 640])\n",
            "t_emb_per_atom shape: torch.Size([24461, 32])\n",
            "x_scalar shape: torch.Size([24461, 672])\n",
            "batch.x_scalar shape: torch.Size([9025, 640])\n",
            "t_emb_per_atom shape: torch.Size([9025, 32])\n",
            "x_scalar shape: torch.Size([9025, 672])\n",
            "Epoch 69/100, Loss: 0.9986855983734131\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_69.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([23457, 640])\n",
            "t_emb_per_atom shape: torch.Size([23457, 32])\n",
            "x_scalar shape: torch.Size([23457, 672])\n",
            "batch.x_scalar shape: torch.Size([29527, 640])\n",
            "t_emb_per_atom shape: torch.Size([29527, 32])\n",
            "x_scalar shape: torch.Size([29527, 672])\n",
            "batch.x_scalar shape: torch.Size([10067, 640])\n",
            "t_emb_per_atom shape: torch.Size([10067, 32])\n",
            "x_scalar shape: torch.Size([10067, 672])\n",
            "Epoch 70/100, Loss: 0.9995658993721008\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_70.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([12070, 640])\n",
            "t_emb_per_atom shape: torch.Size([12070, 32])\n",
            "x_scalar shape: torch.Size([12070, 672])\n",
            "batch.x_scalar shape: torch.Size([17520, 640])\n",
            "t_emb_per_atom shape: torch.Size([17520, 32])\n",
            "x_scalar shape: torch.Size([17520, 672])\n",
            "batch.x_scalar shape: torch.Size([33461, 640])\n",
            "t_emb_per_atom shape: torch.Size([33461, 32])\n",
            "x_scalar shape: torch.Size([33461, 672])\n",
            "Epoch 71/100, Loss: 0.9890846014022827\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_71.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28184, 640])\n",
            "t_emb_per_atom shape: torch.Size([28184, 32])\n",
            "x_scalar shape: torch.Size([28184, 672])\n",
            "batch.x_scalar shape: torch.Size([24800, 640])\n",
            "t_emb_per_atom shape: torch.Size([24800, 32])\n",
            "x_scalar shape: torch.Size([24800, 672])\n",
            "batch.x_scalar shape: torch.Size([10067, 640])\n",
            "t_emb_per_atom shape: torch.Size([10067, 32])\n",
            "x_scalar shape: torch.Size([10067, 672])\n",
            "Epoch 72/100, Loss: 1.0119361877441406\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_72.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([27609, 640])\n",
            "t_emb_per_atom shape: torch.Size([27609, 32])\n",
            "x_scalar shape: torch.Size([27609, 672])\n",
            "batch.x_scalar shape: torch.Size([13988, 640])\n",
            "t_emb_per_atom shape: torch.Size([13988, 32])\n",
            "x_scalar shape: torch.Size([13988, 672])\n",
            "batch.x_scalar shape: torch.Size([21454, 640])\n",
            "t_emb_per_atom shape: torch.Size([21454, 32])\n",
            "x_scalar shape: torch.Size([21454, 672])\n",
            "Epoch 73/100, Loss: 0.9957617521286011\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_73.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28558, 640])\n",
            "t_emb_per_atom shape: torch.Size([28558, 32])\n",
            "x_scalar shape: torch.Size([28558, 672])\n",
            "batch.x_scalar shape: torch.Size([26462, 640])\n",
            "t_emb_per_atom shape: torch.Size([26462, 32])\n",
            "x_scalar shape: torch.Size([26462, 672])\n",
            "batch.x_scalar shape: torch.Size([8031, 640])\n",
            "t_emb_per_atom shape: torch.Size([8031, 32])\n",
            "x_scalar shape: torch.Size([8031, 672])\n",
            "Epoch 74/100, Loss: 0.9939975142478943\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_74.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([14455, 640])\n",
            "t_emb_per_atom shape: torch.Size([14455, 32])\n",
            "x_scalar shape: torch.Size([14455, 672])\n",
            "batch.x_scalar shape: torch.Size([41492, 640])\n",
            "t_emb_per_atom shape: torch.Size([41492, 32])\n",
            "x_scalar shape: torch.Size([41492, 672])\n",
            "batch.x_scalar shape: torch.Size([7104, 640])\n",
            "t_emb_per_atom shape: torch.Size([7104, 32])\n",
            "x_scalar shape: torch.Size([7104, 672])\n",
            "Epoch 75/100, Loss: 0.9989701509475708\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_75.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([42873, 640])\n",
            "t_emb_per_atom shape: torch.Size([42873, 32])\n",
            "x_scalar shape: torch.Size([42873, 672])\n",
            "batch.x_scalar shape: torch.Size([16369, 640])\n",
            "t_emb_per_atom shape: torch.Size([16369, 32])\n",
            "x_scalar shape: torch.Size([16369, 672])\n",
            "batch.x_scalar shape: torch.Size([3809, 640])\n",
            "t_emb_per_atom shape: torch.Size([3809, 32])\n",
            "x_scalar shape: torch.Size([3809, 672])\n",
            "Epoch 76/100, Loss: 0.9975552558898926\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_76.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([33867, 640])\n",
            "t_emb_per_atom shape: torch.Size([33867, 32])\n",
            "x_scalar shape: torch.Size([33867, 672])\n",
            "batch.x_scalar shape: torch.Size([23419, 640])\n",
            "t_emb_per_atom shape: torch.Size([23419, 32])\n",
            "x_scalar shape: torch.Size([23419, 672])\n",
            "batch.x_scalar shape: torch.Size([5765, 640])\n",
            "t_emb_per_atom shape: torch.Size([5765, 32])\n",
            "x_scalar shape: torch.Size([5765, 672])\n",
            "Epoch 77/100, Loss: 0.9906731247901917\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_77.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([41147, 640])\n",
            "t_emb_per_atom shape: torch.Size([41147, 32])\n",
            "x_scalar shape: torch.Size([41147, 672])\n",
            "batch.x_scalar shape: torch.Size([13071, 640])\n",
            "t_emb_per_atom shape: torch.Size([13071, 32])\n",
            "x_scalar shape: torch.Size([13071, 672])\n",
            "batch.x_scalar shape: torch.Size([8833, 640])\n",
            "t_emb_per_atom shape: torch.Size([8833, 32])\n",
            "x_scalar shape: torch.Size([8833, 672])\n",
            "Epoch 78/100, Loss: 0.9802629947662354\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_78.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28446, 640])\n",
            "t_emb_per_atom shape: torch.Size([28446, 32])\n",
            "x_scalar shape: torch.Size([28446, 672])\n",
            "batch.x_scalar shape: torch.Size([11492, 640])\n",
            "t_emb_per_atom shape: torch.Size([11492, 32])\n",
            "x_scalar shape: torch.Size([11492, 672])\n",
            "batch.x_scalar shape: torch.Size([23113, 640])\n",
            "t_emb_per_atom shape: torch.Size([23113, 32])\n",
            "x_scalar shape: torch.Size([23113, 672])\n",
            "Epoch 79/100, Loss: 1.0002124309539795\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_79.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28443, 640])\n",
            "t_emb_per_atom shape: torch.Size([28443, 32])\n",
            "x_scalar shape: torch.Size([28443, 672])\n",
            "batch.x_scalar shape: torch.Size([24541, 640])\n",
            "t_emb_per_atom shape: torch.Size([24541, 32])\n",
            "x_scalar shape: torch.Size([24541, 672])\n",
            "batch.x_scalar shape: torch.Size([10067, 640])\n",
            "t_emb_per_atom shape: torch.Size([10067, 32])\n",
            "x_scalar shape: torch.Size([10067, 672])\n",
            "Epoch 80/100, Loss: 0.9993242025375366\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_80.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([32208, 640])\n",
            "t_emb_per_atom shape: torch.Size([32208, 32])\n",
            "x_scalar shape: torch.Size([32208, 672])\n",
            "batch.x_scalar shape: torch.Size([12067, 640])\n",
            "t_emb_per_atom shape: torch.Size([12067, 32])\n",
            "x_scalar shape: torch.Size([12067, 672])\n",
            "batch.x_scalar shape: torch.Size([18776, 640])\n",
            "t_emb_per_atom shape: torch.Size([18776, 32])\n",
            "x_scalar shape: torch.Size([18776, 672])\n",
            "Epoch 81/100, Loss: 0.9994853138923645\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_81.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([12070, 640])\n",
            "t_emb_per_atom shape: torch.Size([12070, 32])\n",
            "x_scalar shape: torch.Size([12070, 672])\n",
            "batch.x_scalar shape: torch.Size([32205, 640])\n",
            "t_emb_per_atom shape: torch.Size([32205, 32])\n",
            "x_scalar shape: torch.Size([32205, 672])\n",
            "batch.x_scalar shape: torch.Size([18776, 640])\n",
            "t_emb_per_atom shape: torch.Size([18776, 32])\n",
            "x_scalar shape: torch.Size([18776, 672])\n",
            "Epoch 82/100, Loss: 1.0041778087615967\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_82.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([42873, 640])\n",
            "t_emb_per_atom shape: torch.Size([42873, 32])\n",
            "x_scalar shape: torch.Size([42873, 672])\n",
            "batch.x_scalar shape: torch.Size([12032, 640])\n",
            "t_emb_per_atom shape: torch.Size([12032, 32])\n",
            "x_scalar shape: torch.Size([12032, 672])\n",
            "batch.x_scalar shape: torch.Size([8146, 640])\n",
            "t_emb_per_atom shape: torch.Size([8146, 32])\n",
            "x_scalar shape: torch.Size([8146, 672])\n",
            "Epoch 83/100, Loss: 0.9972802996635437\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_83.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([27871, 640])\n",
            "t_emb_per_atom shape: torch.Size([27871, 32])\n",
            "x_scalar shape: torch.Size([27871, 672])\n",
            "batch.x_scalar shape: torch.Size([27836, 640])\n",
            "t_emb_per_atom shape: torch.Size([27836, 32])\n",
            "x_scalar shape: torch.Size([27836, 672])\n",
            "batch.x_scalar shape: torch.Size([7344, 640])\n",
            "t_emb_per_atom shape: torch.Size([7344, 32])\n",
            "x_scalar shape: torch.Size([7344, 672])\n",
            "Epoch 84/100, Loss: 0.9975816607475281\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_84.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([41492, 640])\n",
            "t_emb_per_atom shape: torch.Size([41492, 32])\n",
            "x_scalar shape: torch.Size([41492, 672])\n",
            "batch.x_scalar shape: torch.Size([15832, 640])\n",
            "t_emb_per_atom shape: torch.Size([15832, 32])\n",
            "x_scalar shape: torch.Size([15832, 672])\n",
            "batch.x_scalar shape: torch.Size([5727, 640])\n",
            "t_emb_per_atom shape: torch.Size([5727, 32])\n",
            "x_scalar shape: torch.Size([5727, 672])\n",
            "Epoch 85/100, Loss: 1.0028458833694458\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_85.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([30287, 640])\n",
            "t_emb_per_atom shape: torch.Size([30287, 32])\n",
            "x_scalar shape: torch.Size([30287, 672])\n",
            "batch.x_scalar shape: torch.Size([25081, 640])\n",
            "t_emb_per_atom shape: torch.Size([25081, 32])\n",
            "x_scalar shape: torch.Size([25081, 672])\n",
            "batch.x_scalar shape: torch.Size([7683, 640])\n",
            "t_emb_per_atom shape: torch.Size([7683, 32])\n",
            "x_scalar shape: torch.Size([7683, 672])\n",
            "Epoch 86/100, Loss: 0.9893162250518799\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_86.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([24224, 640])\n",
            "t_emb_per_atom shape: torch.Size([24224, 32])\n",
            "x_scalar shape: torch.Size([24224, 672])\n",
            "batch.x_scalar shape: torch.Size([28763, 640])\n",
            "t_emb_per_atom shape: torch.Size([28763, 32])\n",
            "x_scalar shape: torch.Size([28763, 672])\n",
            "batch.x_scalar shape: torch.Size([10064, 640])\n",
            "t_emb_per_atom shape: torch.Size([10064, 32])\n",
            "x_scalar shape: torch.Size([10064, 672])\n",
            "Epoch 87/100, Loss: 1.007325530052185\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_87.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([11840, 640])\n",
            "t_emb_per_atom shape: torch.Size([11840, 32])\n",
            "x_scalar shape: torch.Size([11840, 672])\n",
            "batch.x_scalar shape: torch.Size([30479, 640])\n",
            "t_emb_per_atom shape: torch.Size([30479, 32])\n",
            "x_scalar shape: torch.Size([30479, 672])\n",
            "batch.x_scalar shape: torch.Size([20732, 640])\n",
            "t_emb_per_atom shape: torch.Size([20732, 32])\n",
            "x_scalar shape: torch.Size([20732, 672])\n",
            "Epoch 88/100, Loss: 0.9980631470680237\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_88.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([25148, 640])\n",
            "t_emb_per_atom shape: torch.Size([25148, 32])\n",
            "x_scalar shape: torch.Size([25148, 672])\n",
            "batch.x_scalar shape: torch.Size([27839, 640])\n",
            "t_emb_per_atom shape: torch.Size([27839, 32])\n",
            "x_scalar shape: torch.Size([27839, 672])\n",
            "batch.x_scalar shape: torch.Size([10064, 640])\n",
            "t_emb_per_atom shape: torch.Size([10064, 32])\n",
            "x_scalar shape: torch.Size([10064, 672])\n",
            "Epoch 89/100, Loss: 1.0038089752197266\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_89.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([12834, 640])\n",
            "t_emb_per_atom shape: torch.Size([12834, 32])\n",
            "x_scalar shape: torch.Size([12834, 672])\n",
            "batch.x_scalar shape: torch.Size([28763, 640])\n",
            "t_emb_per_atom shape: torch.Size([28763, 32])\n",
            "x_scalar shape: torch.Size([28763, 672])\n",
            "batch.x_scalar shape: torch.Size([21454, 640])\n",
            "t_emb_per_atom shape: torch.Size([21454, 32])\n",
            "x_scalar shape: torch.Size([21454, 672])\n",
            "Epoch 90/100, Loss: 0.9919114112854004\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_90.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([28561, 640])\n",
            "t_emb_per_atom shape: torch.Size([28561, 32])\n",
            "x_scalar shape: torch.Size([28561, 672])\n",
            "batch.x_scalar shape: torch.Size([12419, 640])\n",
            "t_emb_per_atom shape: torch.Size([12419, 32])\n",
            "x_scalar shape: torch.Size([12419, 672])\n",
            "batch.x_scalar shape: torch.Size([22071, 640])\n",
            "t_emb_per_atom shape: torch.Size([22071, 32])\n",
            "x_scalar shape: torch.Size([22071, 672])\n",
            "Epoch 91/100, Loss: 0.9938639998435974\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_91.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([27181, 640])\n",
            "t_emb_per_atom shape: torch.Size([27181, 32])\n",
            "x_scalar shape: torch.Size([27181, 672])\n",
            "batch.x_scalar shape: torch.Size([31486, 640])\n",
            "t_emb_per_atom shape: torch.Size([31486, 32])\n",
            "x_scalar shape: torch.Size([31486, 672])\n",
            "batch.x_scalar shape: torch.Size([4384, 640])\n",
            "t_emb_per_atom shape: torch.Size([4384, 32])\n",
            "x_scalar shape: torch.Size([4384, 672])\n",
            "Epoch 92/100, Loss: 1.0068726539611816\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_92.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([25842, 640])\n",
            "t_emb_per_atom shape: torch.Size([25842, 32])\n",
            "x_scalar shape: torch.Size([25842, 672])\n",
            "batch.x_scalar shape: torch.Size([13409, 640])\n",
            "t_emb_per_atom shape: torch.Size([13409, 32])\n",
            "x_scalar shape: torch.Size([13409, 672])\n",
            "batch.x_scalar shape: torch.Size([23800, 640])\n",
            "t_emb_per_atom shape: torch.Size([23800, 32])\n",
            "x_scalar shape: torch.Size([23800, 672])\n",
            "Epoch 93/100, Loss: 1.0015231370925903\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_93.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([16756, 640])\n",
            "t_emb_per_atom shape: torch.Size([16756, 32])\n",
            "x_scalar shape: torch.Size([16756, 672])\n",
            "batch.x_scalar shape: torch.Size([24224, 640])\n",
            "t_emb_per_atom shape: torch.Size([24224, 32])\n",
            "x_scalar shape: torch.Size([24224, 672])\n",
            "batch.x_scalar shape: torch.Size([22071, 640])\n",
            "t_emb_per_atom shape: torch.Size([22071, 32])\n",
            "x_scalar shape: torch.Size([22071, 672])\n",
            "Epoch 94/100, Loss: 1.0030641555786133\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_94.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([31946, 640])\n",
            "t_emb_per_atom shape: torch.Size([31946, 32])\n",
            "x_scalar shape: torch.Size([31946, 672])\n",
            "batch.x_scalar shape: torch.Size([23422, 640])\n",
            "t_emb_per_atom shape: torch.Size([23422, 32])\n",
            "x_scalar shape: torch.Size([23422, 672])\n",
            "batch.x_scalar shape: torch.Size([7683, 640])\n",
            "t_emb_per_atom shape: torch.Size([7683, 32])\n",
            "x_scalar shape: torch.Size([7683, 672])\n",
            "Epoch 95/100, Loss: 1.00322425365448\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_95.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([26120, 640])\n",
            "t_emb_per_atom shape: torch.Size([26120, 32])\n",
            "x_scalar shape: torch.Size([26120, 672])\n",
            "batch.x_scalar shape: torch.Size([29827, 640])\n",
            "t_emb_per_atom shape: torch.Size([29827, 32])\n",
            "x_scalar shape: torch.Size([29827, 672])\n",
            "batch.x_scalar shape: torch.Size([7104, 640])\n",
            "t_emb_per_atom shape: torch.Size([7104, 32])\n",
            "x_scalar shape: torch.Size([7104, 672])\n",
            "Epoch 96/100, Loss: 1.0040318965911865\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_96.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([25883, 640])\n",
            "t_emb_per_atom shape: torch.Size([25883, 32])\n",
            "x_scalar shape: torch.Size([25883, 672])\n",
            "batch.x_scalar shape: torch.Size([27104, 640])\n",
            "t_emb_per_atom shape: torch.Size([27104, 32])\n",
            "x_scalar shape: torch.Size([27104, 672])\n",
            "batch.x_scalar shape: torch.Size([10064, 640])\n",
            "t_emb_per_atom shape: torch.Size([10064, 32])\n",
            "x_scalar shape: torch.Size([10064, 672])\n",
            "Epoch 97/100, Loss: 1.0021753311157227\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_97.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([30105, 640])\n",
            "t_emb_per_atom shape: torch.Size([30105, 32])\n",
            "x_scalar shape: torch.Size([30105, 672])\n",
            "batch.x_scalar shape: torch.Size([11492, 640])\n",
            "t_emb_per_atom shape: torch.Size([11492, 32])\n",
            "x_scalar shape: torch.Size([11492, 672])\n",
            "batch.x_scalar shape: torch.Size([21454, 640])\n",
            "t_emb_per_atom shape: torch.Size([21454, 32])\n",
            "x_scalar shape: torch.Size([21454, 672])\n",
            "Epoch 98/100, Loss: 0.9942668080329895\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_98.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([24224, 640])\n",
            "t_emb_per_atom shape: torch.Size([24224, 32])\n",
            "x_scalar shape: torch.Size([24224, 672])\n",
            "batch.x_scalar shape: torch.Size([16756, 640])\n",
            "t_emb_per_atom shape: torch.Size([16756, 32])\n",
            "x_scalar shape: torch.Size([16756, 672])\n",
            "batch.x_scalar shape: torch.Size([22071, 640])\n",
            "t_emb_per_atom shape: torch.Size([22071, 32])\n",
            "x_scalar shape: torch.Size([22071, 672])\n",
            "Epoch 99/100, Loss: 1.0011565685272217\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_99.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
            "batch.x_scalar shape: torch.Size([13074, 640])\n",
            "t_emb_per_atom shape: torch.Size([13074, 32])\n",
            "x_scalar shape: torch.Size([13074, 672])\n",
            "batch.x_scalar shape: torch.Size([26177, 640])\n",
            "t_emb_per_atom shape: torch.Size([26177, 32])\n",
            "x_scalar shape: torch.Size([26177, 672])\n",
            "batch.x_scalar shape: torch.Size([23800, 640])\n",
            "t_emb_per_atom shape: torch.Size([23800, 32])\n",
            "x_scalar shape: torch.Size([23800, 672])\n",
            "Epoch 100/100, Loss: 1.0026001930236816\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved standard checkpoint to /content/drive/MyDrive/model_epoch_100.pth\n",
            "Failed to save TorchScript checkpoint: Type 'Tuple[Tuple[str, Tensor], Tuple[str, Tensor], Tuple[str, Tensor]]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------TPU (currently not working)---------------------------\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import knn_graph\n",
        "from transformers import EsmModel, EsmTokenizer\n",
        "from diffusers import DDPMScheduler\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from Bio.PDB import PPBuilder, MMCIFParser\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import numpy as np\n",
        "import torch_cluster\n",
        "\n",
        "# Args class\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.data_dir = \"data\"\n",
        "        self.device = \"tpu\"  # Updated to TPU\n",
        "        self.epochs = 100\n",
        "\n",
        "# Placeholder for timestep_embedding (adjust if different)\n",
        "def timestep_embedding(t, dim):\n",
        "    return torch.sin(t.view(-1, 1) * torch.linspace(0, 1, dim // 2, device=t.device)).repeat(1, 2)\n",
        "\n",
        "# EGNNLayer (assumed, simplified for JIT compatibility)\n",
        "class EGNNLayer(nn.Module):\n",
        "    def __init__(self, in_scalar_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.scalar_net = nn.Linear(in_scalar_dim, hidden_dim)\n",
        "        self.message_net = nn.Linear(hidden_dim + 3, hidden_dim)\n",
        "        self.update_net = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, h, x, edge_index):\n",
        "        h = F.relu(self.scalar_net(h))\n",
        "        row, col = edge_index\n",
        "        dist = torch.norm(x[row] - x[col], dim=-1, keepdim=True)\n",
        "        msg = F.relu(self.message_net(torch.cat([h[row], dist], dim=-1)))\n",
        "        aggr = torch.zeros_like(h).scatter_add_(0, col.view(-1, 1).expand(-1, h.size(1)), msg)\n",
        "        h = F.relu(self.update_net(aggr))\n",
        "        return h, x\n",
        "\n",
        "# GraphUNet with gradient checkpointing\n",
        "class GraphUNet(nn.Module):\n",
        "    def __init__(self, in_scalar_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.down1 = EGNNLayer(in_scalar_dim, hidden_dim)\n",
        "        self.down2 = EGNNLayer(hidden_dim, hidden_dim)\n",
        "        self.bottleneck = EGNNLayer(hidden_dim, hidden_dim)\n",
        "        self.up2 = EGNNLayer(hidden_dim * 2, hidden_dim)\n",
        "        self.up1 = EGNNLayer(hidden_dim * 2, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, 3)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x_scalar, x_vector, edge_index = data.x_scalar, data.x_vector, data.edge_index\n",
        "        h1, _ = checkpoint(self.down1, x_scalar, x_vector, edge_index)\n",
        "        h2, _ = checkpoint(self.down2, h1, x_vector, edge_index)\n",
        "        h_b, _ = checkpoint(self.bottleneck, h2, x_vector, edge_index)\n",
        "        h_up2, _ = checkpoint(self.up2, torch.cat([h_b, h2], dim=1), x_vector, edge_index)\n",
        "        h_up1, _ = checkpoint(self.up1, torch.cat([h_up2, h1], dim=1), x_vector, edge_index)\n",
        "        pred_epsilon = self.out(h_up1)\n",
        "        return pred_epsilon\n",
        "\n",
        "# ProteinDataset (assumed, minimal for loading .pkl files)\n",
        "class ProteinDataset:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.data = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.pkl')]\n",
        "        self.data = [pickle.load(open(f, 'rb')) for f in self.data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "#parse cif\n",
        "def parse_cif(cif_file):\n",
        "    parser = MMCIFParser(QUIET=True)\n",
        "    structure = parser.get_structure('protein', cif_file)\n",
        "    ppb = PPBuilder()\n",
        "    peptides = ppb.build_peptides(structure)\n",
        "    sequence = ''.join([str(pp.get_sequence()) for pp in ppb.build_peptides(structure)])\n",
        "    seq_residues = []\n",
        "    for pp in peptides:\n",
        "        seq_residues.extend(pp)\n",
        "\n",
        "    # Create a set of residue IDs for filtering\n",
        "    seq_residue_ids = set(res.get_id() for res in seq_residues)\n",
        "    coords = []\n",
        "    valid_atoms=[]\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "              if residue.get_id() in seq_residue_ids:\n",
        "                for atom in residue:\n",
        "                    coords.append(atom.get_coord())\n",
        "                    valid_atoms.append(residue.get_id())\n",
        "\n",
        "    coords = torch.tensor(np.array(coords), dtype=torch.float32)\n",
        "    return sequence, coords, valid_atoms\n",
        "\n",
        "\n",
        "# Preprocess data (unchanged, outputs 640-dimensional embeddings)\n",
        "def preprocess_data(data_dir):\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t30_150M_UR50D\")\n",
        "    esm_model = EsmModel.from_pretrained(\"facebook/esm2_t30_150M_UR50D\", ignore_mismatched_sizes=True).eval()\n",
        "    print(f\"ESM-2 hidden size: {esm_model.config.hidden_size}\")  # Expect 640 per user\n",
        "\n",
        "    cif_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.cif')]\n",
        "    if not cif_files:\n",
        "        print(f\"No .cif files found in {data_dir}\")\n",
        "        return\n",
        "\n",
        "    for cif_file in cif_files:\n",
        "        output_file = os.path.join(data_dir, os.path.basename(cif_file).replace('.cif', '.pkl'))\n",
        "        if os.path.exists(output_file):\n",
        "            print(f\"Skipping {cif_file}, already preprocessed.\")\n",
        "            continue\n",
        "        print(f\"Processing {cif_file}\")\n",
        "        sequence, coords, valid_atoms = parse_cif(cif_file)\n",
        "        inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = esm_model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state[0]  # [seq_len, 640]\n",
        "        print(f\"Embeddings shape before mapping: {embeddings.shape}\")\n",
        "\n",
        "        seq_residues = []\n",
        "        for pp in PPBuilder().build_peptides(MMCIFParser().get_structure('protein', cif_file)):\n",
        "            seq_residues.extend(pp)\n",
        "        residue_id_to_idx = {res.get_id(): idx for idx, res in enumerate(seq_residues)}\n",
        "\n",
        "        atom_to_residue = []\n",
        "        for res_id in valid_atoms:\n",
        "            if res_id in residue_id_to_idx:\n",
        "                atom_to_residue.append(residue_id_to_idx[res_id])\n",
        "            else:\n",
        "                print(f\"Warning: Residue ID {res_id} not found in sequence for {cif_file}\")\n",
        "\n",
        "        if not atom_to_residue:\n",
        "            print(f\"Error: No valid atom-to-residue mappings for {cif_file}\")\n",
        "            continue\n",
        "        if max(atom_to_residue, default=-1) >= len(embeddings):\n",
        "            print(f\"Error: Residue index out of bounds for {cif_file}\")\n",
        "            continue\n",
        "        embeddings = embeddings[atom_to_residue]\n",
        "        print(f\"Embeddings shape after mapping: {embeddings.shape}\")\n",
        "        data = Data(\n",
        "            x_scalar=embeddings,\n",
        "            x_vector=coords,\n",
        "            edge_index=knn_graph(coords, k=6)\n",
        "        )\n",
        "        with open(output_file, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"Saved preprocessed data to {output_file}\")\n",
        "\n",
        "# Train function with TPU, gradient checkpointing, and TorchScript\n",
        "def train(args):\n",
        "    # Set up TPU device\n",
        "    device = xm.xla_device()\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize dataset and loader\n",
        "    dataset = ProteinDataset(args.data_dir)\n",
        "    if len(dataset) == 0:\n",
        "        print(f\"No preprocessed .pkl files found in {args.data_dir}. Please run preprocessing first.\")\n",
        "        return\n",
        "    loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=Batch.from_data_list)\n",
        "\n",
        "    # Wrap loader for TPU\n",
        "    xm_loader = pl.MpDeviceLoader(loader, device)\n",
        "\n",
        "    # Initialize model and move to TPU\n",
        "    model = GraphUNet(in_scalar_dim=640 + 32, hidden_dim=256).to(device)\n",
        "    scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "    scheduler.alphas_cumprod = scheduler.alphas_cumprod.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Ensure checkpoint directory\n",
        "    os.makedirs(args.data_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch in xm_loader:\n",
        "            batch = batch.to(device)\n",
        "            t = torch.randint(0, scheduler.config.num_train_timesteps, (batch.num_graphs,), device=device)\n",
        "            alpha_bar = scheduler.alphas_cumprod[t]\n",
        "            alpha_bar_per_atom = alpha_bar[batch.batch]\n",
        "            sqrt_alpha_bar = torch.sqrt(alpha_bar_per_atom)\n",
        "            sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar_per_atom)\n",
        "            noise = torch.randn_like(batch.x_vector, device=device)\n",
        "            x_t = sqrt_alpha_bar[:, None] * batch.x_vector + sqrt_one_minus_alpha_bar[:, None] * noise\n",
        "            t_emb = timestep_embedding(t, dim=32).to(device)\n",
        "            t_emb_per_atom = t_emb[batch.batch]\n",
        "            x_scalar = torch.cat([batch.x_scalar, t_emb_per_atom], dim=1)\n",
        "            data = Data(x_scalar=x_scalar, x_vector=x_t, edge_index=batch.edge_index)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred_epsilon = model(data)\n",
        "            loss = F.mse_loss(pred_epsilon, noise)\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch + 1}/{args.epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save standard checkpoint\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        # Update checkpoint_path\n",
        "        checkpoint_path = os.path.join('/content/drive/MyDrive/', f\"model_epoch_{epoch + 1}.pth\")\n",
        "        jit_checkpoint_path = os.path.join('/content/drive/MyDrive/', f\"model_epoch_{epoch + 1}_jit.pt\")\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Saved standard checkpoint to {checkpoint_path}\")\n",
        "\n",
        "        # Save TorchScript checkpoint\n",
        "        model.eval()  # JIT requires eval mode\n",
        "        try:\n",
        "            # Create example input for tracing\n",
        "            example_data = Data(\n",
        "                x_scalar=torch.randn(100, 640 + 32, device=device),\n",
        "                x_vector=torch.randn(100, 3, device=device),\n",
        "                edge_index=torch.randint(0, 100, (2, 200), device=device)\n",
        "            )\n",
        "            # Trace the model\n",
        "            traced_model = torch.jit.trace(model, example_data)\n",
        "            traced_model.save(jit_checkpoint_path)\n",
        "            print(f\"Saved TorchScript checkpoint to {jit_checkpoint_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save TorchScript checkpoint: {e}\")\n",
        "        model.train()\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    args = Args()\n",
        "    preprocess_data(args.data_dir)\n",
        "    train(args)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_H6nraFFtnwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INFERENCE"
      ],
      "metadata": {
        "id": "wY8aYs-q6a6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import knn_graph\n",
        "from transformers import EsmModel\n",
        "import numpy as np\n",
        "try:\n",
        "    from Bio.PDB.MMCIFIO import MMCIFIO\n",
        "except ImportError:\n",
        "    print(\"Warning: MMCIFIO not available. Falling back to PDB output.\")\n",
        "    MMCIFIO = None\n",
        "from Bio.PDB import Structure, Model, Chain, Residue, Atom, PDBIO\n",
        "from Bio.PDB.MMCIFParser import MMCIFParser\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Timestep embedding\n",
        "def timestep_embedding(t, dim):\n",
        "    return torch.sin(t.view(-1, 1) * torch.linspace(0, 1, dim // 2, device=t.device)).repeat(1, 2)\n",
        "\n",
        "# EGNNLayer\n",
        "class EGNNLayer(nn.Module):\n",
        "    def __init__(self, in_scalar_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.scalar_net = nn.Linear(in_scalar_dim, hidden_dim)\n",
        "        self.vector_net = nn.Linear(hidden_dim , hidden_dim)\n",
        "\n",
        "    def forward(self, h, x, edge_index):\n",
        "        h = F.relu(self.scalar_net(h))\n",
        "        return h, x\n",
        "\n",
        "# GraphUNet\n",
        "class GraphUNet(nn.Module):\n",
        "    def __init__(self, in_scalar_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.down1 = EGNNLayer(in_scalar_dim, hidden_dim)\n",
        "        self.down2 = EGNNLayer(hidden_dim, hidden_dim)\n",
        "        self.bottleneck = EGNNLayer(hidden_dim, hidden_dim)\n",
        "        self.up2 = EGNNLayer(hidden_dim * 2, hidden_dim)\n",
        "        self.up1 = EGNNLayer(hidden_dim * 2, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, 3)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x_scalar, x_vector, edge_index = data.x_scalar, data.x_vector, data.edge_index\n",
        "        h1, x1 = self.down1(x_scalar, x_vector, edge_index)\n",
        "        h2, x2 = self.down2(h1, x1, edge_index)\n",
        "        h_b, x_b = self.bottleneck(h2, x2, edge_index)\n",
        "        h_up2, x_up2 = self.up2(torch.cat([h_b, h2], dim=1), x_b, edge_index)\n",
        "        h_up1, x_up1 = self.up1(torch.cat([h_up2, h1], dim=1), x_up2, edge_index)\n",
        "        pred_epsilon = self.out(h_up1)\n",
        "        return pred_epsilon\n",
        "\n",
        "# Inference function\n",
        "def infer_from_sequence(sequence, checkpoint_path, output_cif_path, num_timesteps=1000):\n",
        "    # Validate sequence\n",
        "    valid_aas = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "    if not sequence or not all(aa in valid_aas for aa in sequence):\n",
        "        raise ValueError(\"Invalid sequence. Use single-letter codes: ACDEFGHIKLMNPQRSTVWY\")\n",
        "\n",
        "    # Initialize model and scheduler\n",
        "    model = GraphUNet(in_scalar_dim=640 + 32, hidden_dim=256).to(device)\n",
        "    scheduler = DDPMScheduler(num_train_timesteps=num_timesteps)\n",
        "\n",
        "    # Load checkpoint\n",
        "    if checkpoint_path.endswith('.pth'):\n",
        "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    elif checkpoint_path.endswith('.pt'):\n",
        "        model = torch.jit.load(checkpoint_path, map_location=device)\n",
        "    else:\n",
        "        raise ValueError(\"Checkpoint must be .pth or .pt\")\n",
        "    model.eval()\n",
        "\n",
        "    # Generate ESM-2 embeddings\n",
        "    tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t30_150M_UR50D\")\n",
        "    esm_model = EsmModel.from_pretrained(\"facebook/esm2_t30_150M_UR50D\").to(device).eval()\n",
        "    inputs = tokenizer(sequence, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = esm_model(**inputs)\n",
        "    x_scalar = outputs.last_hidden_state[0, 1:-1, :]  # [seq_len, 640]\n",
        "    num_residues = x_scalar.size(0)\n",
        "    if num_residues != len(sequence):\n",
        "        raise ValueError(f\"Embedding size {num_residues} does not match sequence length {len(sequence)}\")\n",
        "\n",
        "    # Initialize noisy coordinates\n",
        "    x_t = torch.randn(num_residues, 3, device=device)\n",
        "    edge_index = knn_graph(x_t, k=6).to(device)\n",
        "    batch = torch.zeros(num_residues, dtype=torch.long, device=device)\n",
        "\n",
        "    # Denoising loop\n",
        "    with torch.no_grad():\n",
        "        for t in reversed(range(num_timesteps)):\n",
        "            t_tensor = torch.full((1,), t, dtype=torch.long, device=device)\n",
        "            t_emb = timestep_embedding(t_tensor, dim=32)\n",
        "            t_emb_per_atom = t_emb.repeat(num_residues, 1)\n",
        "            x_scalar_t = torch.cat([x_scalar, t_emb_per_atom], dim=1)\n",
        "\n",
        "            if t % 200 == 0:\n",
        "                edge_index = knn_graph(x_t, k=6).to(device)\n",
        "\n",
        "            data_t = Data(x_scalar=x_scalar_t, x_vector=x_t, edge_index=edge_index)\n",
        "            pred_epsilon = model(data_t)\n",
        "\n",
        "            # Clip pred_epsilon to prevent explosions\n",
        "            pred_epsilon = torch.clamp(pred_epsilon, -10.0, 10.0)\n",
        "\n",
        "            noise_scale = torch.sqrt(1 - scheduler.alphas_cumprod[t]).to(device)\n",
        "            signal_scale = torch.sqrt(scheduler.alphas_cumprod[t]).to(device)\n",
        "            x_t = (x_t - noise_scale * pred_epsilon) / signal_scale if t > 0 else x_t - pred_epsilon\n",
        "\n",
        "            # Normalize x_t to prevent divergence\n",
        "            x_t = x_t / (x_t.norm(dim=-1, keepdim=True) + 1e-8) * 10.0  # Scale to ~10\n",
        "\n",
        "            # Check for inf/nan\n",
        "            if torch.isnan(x_t).any() or torch.isinf(x_t).any():\n",
        "                print(f\"Warning: inf/nan detected at step {t}\")\n",
        "                break\n",
        "\n",
        "            if t % 100 == 0:\n",
        "                print(f\"Denoising step {t}/{num_timesteps}, x_t mean: {x_t.mean().item():.4f}\")\n",
        "\n",
        "    # Final coordinates\n",
        "    pred_coords = x_t.cpu()\n",
        "\n",
        "    # Validate coordinates\n",
        "    if torch.isnan(pred_coords).any() or torch.isinf(pred_coords).any():\n",
        "        raise ValueError(\"Invalid coordinates: inf/nan detected\")\n",
        "\n",
        "    # Normalize to reasonable scale\n",
        "    pred_coords = (pred_coords - pred_coords.mean(dim=0)) / (pred_coords.std(dim=0) + 1e-8) * 3.8  # ~3.8 CA-CA distance\n",
        "\n",
        "    # Create structure\n",
        "    structure = Structure.Structure('protein')\n",
        "    model = Model.Model(0)\n",
        "    chain = Chain.Chain('A')\n",
        "    for i, aa in enumerate(sequence, 1):\n",
        "        res = Residue.Residue((' ', i, ' '), aa, ' ')\n",
        "        atom = Atom.Atom('CA', pred_coords[i-1].numpy(), 0.0, 1.0, ' ', 'CA', i)\n",
        "        res.add(atom)\n",
        "        chain.add(res)\n",
        "    model.add(chain)\n",
        "    structure.add(model)\n",
        "\n",
        "    # Save outputs\n",
        "    output_pdb_path = output_cif_path.replace('.cif', '.pdb')\n",
        "    output_npy_path = output_cif_path.replace('.cif', '.npy')\n",
        "\n",
        "    if MMCIFIO is not None:\n",
        "        try:\n",
        "            io = MMCIFIO()\n",
        "            io.set_structure(structure)\n",
        "            io.save(output_cif_path)\n",
        "            print(f\"Saved predicted structure to {output_cif_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save CIF: {e}\")\n",
        "    else:\n",
        "        print(\"MMCIFIO unavailable, skipping CIF output.\")\n",
        "\n",
        "    try:\n",
        "        io = PDBIO()\n",
        "        io.set_structure(structure)\n",
        "        io.save(output_pdb_path)\n",
        "        print(f\"Saved predicted structure to {output_pdb_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save PDB: {e}\")\n",
        "\n",
        "    np.save(output_npy_path, pred_coords.numpy())\n",
        "    print(f\"Saved raw coordinates to {output_npy_path}\")\n",
        "\n",
        "    return pred_coords\n",
        "\n",
        "# Example usage\n",
        "sequence = \"CDAFVGTWKLVSSENFDDYMKEVGVGFATRKVAGMAKPNMIISVNGDLVTIRSESTFKNT\"  # First 60 residues from your PDB\n",
        "checkpoint_path = \"/content/drive/MyDrive/model_epoch_100.pth\"\n",
        "output_cif_path = \"./predicted_structure2.cif\"\n",
        "pred_coords = infer_from_sequence(sequence, checkpoint_path, output_cif_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtZUefEX-GgV",
        "outputId": "1827aebb-ec2f-43ce-d069-b96b87a38a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: MMCIFIO not available. Falling back to PDB output.\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Denoising step 900/1000, x_t mean: -0.4875\n",
            "Denoising step 800/1000, x_t mean: -0.4984\n",
            "Denoising step 700/1000, x_t mean: -0.5484\n",
            "Denoising step 600/1000, x_t mean: -0.5607\n",
            "Denoising step 500/1000, x_t mean: -0.6061\n",
            "Denoising step 400/1000, x_t mean: -0.6205\n",
            "Denoising step 300/1000, x_t mean: -0.6081\n",
            "Denoising step 200/1000, x_t mean: -0.5845\n",
            "Denoising step 100/1000, x_t mean: -0.5853\n",
            "Denoising step 0/1000, x_t mean: -0.5809\n",
            "MMCIFIO unavailable, skipping CIF output.\n",
            "Saved predicted structure to ./predicted_structure2.pdb\n",
            "Saved raw coordinates to ./predicted_structure2.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm data/*.pkl"
      ],
      "metadata": {
        "id": "4V5fsxgwmo_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ~/.cache/huggingface/"
      ],
      "metadata": {
        "id": "Acg0mQ70qYVy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}